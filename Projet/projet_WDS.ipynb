{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADMDWc5LgpC0"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# WSD par fine-tuning d'un modèle *BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUrBv8Gvg2bE"
   },
   "source": [
    "On utilise ici les données du French FrameNet \"ASFALDA\": dans ces données, certains mots ont été associés à un frame FrameNet\n",
    "- ces mots sont les \"targets\"\n",
    "- **associer le bon frame à un target correspond à une tâche de WSD**\n",
    "- modulo le fait qu'un même frame regroupe plusieurs entrées lexicales (par exemple FR_Commerce_buy => acheter.v, achat.n, acquérir.v, etc...)\n",
    "- dans les données, les phrases contenant plusieurs targets ont été dupliquées: on a une ligne par couple phrase + target\n",
    "\n",
    "Les données FrameNet comprennent également l'annotation des arguments sémantiques, et leur typage au moyen d'un rôle (Buyer, Seller, Goods ...), que l'on ignorera ici.\n",
    "\n",
    "On va construire un classifieur :\n",
    "- entrée = un target et sa phrase de contexte\n",
    "- sortie = une distribution de probas sur les différents sens\n",
    "  - ici les sens sont des frames\n",
    "  - on peut ou pas contraindre que les sens \"permis\" pour un target soient uniquement ceux vus à l'entraîînement pour ce target (pour ce lemme)\n",
    "\n",
    "On utilisera un modèle *BERT pour obtenir une représentation contextuelle du mot target.\n",
    "\n",
    "Mais BERT donne des vecteurs contextuels pour chaque **token**, un token pouvant être un sous-mot.\n",
    "**Dans la version de base, vous utiliserez le vecteur *BERT du premier token du mot target.**\n",
    "\n",
    "Ainsi pour le target *comprenions* dans:\n",
    "\n",
    "*Nous comprenions bien le cours*\n",
    "\n",
    "tokenisé en :\n",
    "\n",
    "'\\<s>', 'Nous\\</w>', 'compren', 'ions\\</w>', 'bien\\</w>', 'le\\</w>', 'cours\\</w>', '.\\</w>, '\\</s>'\n",
    "\n",
    "vous utiliserez le vecteur caché du sous-mot \"compren\".\n",
    "\n",
    "Le classifieur dans la version de base sera un réseau de neurones constitué\n",
    "- d'un réseau *BERT\n",
    "- dont on récupère le vecteur caché du 1er sous-mot du target\n",
    "- et une couche linéaire + softmax sur les différents frames présents dans les données d'entraînement.\n",
    "\n",
    "Le classifieur est unique pour tous les lemmes, et peut prédire tout sens(frame) pour tout lemme target, même s'il s'agit d'un sens non vu pour ce lemme dans les données d'apprentissage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "xp3IN9YQexxx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from tqdm import tqdm  \n",
    "from tqdm.notebook import tqdm # for progress bars in notebooks\n",
    "from random import shuffle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_c3C5Pzexx-"
   },
   "source": [
    "## Conventions de nommage des variables\n",
    "\n",
    "- on considère des phrases déjà segmentées en mots (avec segmenteur par règles)\n",
    "- (mais pas encore segmentées en sous-mots)\n",
    "- on utilise \"word\" ou \"w\" pour un mot (ou ponctuation)\n",
    "- et \"token\" après tokenisation de type *BERT (BPE ou WordPiece etc...)\n",
    "\n",
    "- on distingue dans les noms de variables \n",
    " - les identifiants entiers des symboles \n",
    "   (pour le vocabulaire des tokens, le vocabulaire des labels ...)\n",
    " - versus le rang d'un élément (token ou mot) dans une séquence\n",
    "- tid => id de token\n",
    "- trk / wrk => rang de token / rang de mot dans une séquence\n",
    "- tg => \"target\", donc \n",
    " - tg_wrk = le rang dans la phrase du mot target\n",
    " - tg_trk = le rang dans la tokenisation *BERT du premier token du mot target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfm0CXb9exyF"
   },
   "source": [
    "## Les données \"ASFALDA\"\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Il s'agit des données d'un FrameNet du français, comprenant environ 16000 annotations, pour environ 100 frames distincts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kySX0ye3jdpP"
   },
   "source": [
    "### Récupération des données\n",
    "\n",
    "> Indented block\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_2cjFvYfOn3"
   },
   "source": [
    "### Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "fYQhZaBzexyF"
   },
   "outputs": [],
   "source": [
    "# lecture des données\n",
    "\n",
    "def load_asfalda_data(gold_data_file, split_info_file, val_proportion=None):\n",
    "    \"\"\"\n",
    "        Inputs: - asfalda gold data file\n",
    "                - file with list of sentid / corpus type pairs (corpus types train/dev/test)\n",
    "                - val_proportion : if set to value > 0 (and <1)\n",
    "                  the training file is split into train/validation\n",
    "                  so that the validation part represents the provided proportion \n",
    "                  of the original training file\n",
    "        Returns 3 dictionaries (whose keys are corpus types (train/dev/test/val))\n",
    "        - sentences\n",
    "        - rank of target word in each sentence\n",
    "        - gold labels\n",
    "\n",
    "        Example:\n",
    "        sentences['train'] = [['Le', 'code', 'comprend', 'des', 'erreurs','.'],\n",
    "                              ['Comprends', '-tu', '?']]\n",
    "         # the targets are are the 3rd and first words                     \n",
    "        tg_wrks['train'] = [2, 0]\n",
    "        tg_lemmas['train'] = ['comprendre', 'comprendre']\n",
    "        labels = ['frame1', 'frame2']\n",
    "                                \n",
    "    \"\"\"\n",
    "    # chargement de la répartition usuelle des phrases en train / dev / test\n",
    "    s = open(split_info_file)\n",
    "    lines = [ l[:-1].split('\\t') for l in s.readlines() ]\n",
    "    split_info_dic = { line[0]:line[1] for line in lines }\n",
    "\n",
    "    # les phrases de dev / train / test\n",
    "    sentences = {'dev':[], 'train':[], 'test':[]}\n",
    "    # les word rank (wrk) des mots étiquetés en frames (les \"targets\" ou \"tg\")\n",
    "    tg_wrks = {'dev':[], 'train':[], 'test':[]}\n",
    "    # les lemmes des targets\n",
    "    tg_lemmas = {'dev':[], 'train':[], 'test':[]}\n",
    "    # les sens (= des frames) étiquetés pour ces mots\n",
    "    labels = {'dev':[], 'train':[], 'test':[]}\n",
    "\n",
    "    max_sent_len = {'dev':0, 'train':0, 'test':0}\n",
    "    max_tg_wrk = {'dev':0, 'train':0, 'test':0}\n",
    "\n",
    "    stream = open(gold_data_file)\n",
    "    for line in stream.readlines():\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        (sentid, tg_wrk, frame_name, tg_lemma, tg_pos, rest) = line.split('\\t',5)\n",
    "        # on ignore pour l'instant l'annotation en rôles\n",
    "        # les phrases sont pré-segmentées en mots (séparateur = espace) \n",
    "        # => on splitte, de manière à utiliser infra le tokenizer en mode is_split_into_words=True\n",
    "        sentence = rest.split(\"\\t\")[-1].split(' ')\n",
    "        part = split_info_dic[sentid]\n",
    "        tg_wrk = int(tg_wrk)\n",
    "\n",
    "        l = len(sentence)\n",
    "        sentences[part].append(sentence)\n",
    "        labels[part].append(frame_name)\n",
    "        tg_wrks[part].append(tg_wrk)\n",
    "        tg_lemmas[part].append(tg_lemma)\n",
    "        if max_sent_len[part] < l: \n",
    "            max_sent_len[part] = l \n",
    "        if max_tg_wrk[part] < tg_wrk: \n",
    "            max_tg_wrk[part] = tg_wrk \n",
    "    print(\"Longueur max des phrases:\", max_sent_len)\n",
    "    print(\"Rang max du target (en mots):\", max_tg_wrk)\n",
    "    \n",
    "    # decoupage du train en train + validation\n",
    "    # (pour réglage du nombre d'époques)\n",
    "    if val_proportion:\n",
    "        # le split sera le même pour les 3 listes\n",
    "        for dic in [sentences, tg_wrks, labels, tg_lemmas]:\n",
    "            (dic['val'], dic['train']) = split_list(dic['train'], proportion=val_proportion)\n",
    "    return sentences, tg_wrks, tg_lemmas, labels\n",
    "\n",
    "def split_list(inlist, proportion=0.1, shuffle=False):\n",
    "     \"\"\" partitions the input list of items (of any kind) into 2 lists, \n",
    "     the first one representing @proportion of the whole \n",
    "     \n",
    "     If shuffle is not set, the partition takes one item every xxx items\n",
    "     otherwise, the split is random\"\"\"\n",
    "     n = len(inlist)\n",
    "     size1 = int(n * proportion)\n",
    "     if not(size1):\n",
    "          size1 = 1\n",
    "     print(\"SPLIT %d items into %d and %d\" % (n, n-size1, size1))\n",
    "     # if shuffle : simply shuffle and return slices\n",
    "     if shuffle:\n",
    "          # shuffle inlist (without changing the original external list\n",
    "          # use of random.sample instead of random.shuffle\n",
    "          inlist = sample(inlist, n)\n",
    "          return (inlist[:size1], inlist[size1:])\n",
    "     # otherwise, return validation set as one out of xxx items\n",
    "     else:\n",
    "          divisor = int(n / size1)\n",
    "          l1 = []\n",
    "          l2 = []\n",
    "          for (i,x) in enumerate(inlist):\n",
    "               if i % divisor or len(l1) >= size1:\n",
    "                    l2.append(x)\n",
    "               else:\n",
    "                    l1.append(x)\n",
    "          return (l1,l2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QaKnFVJ0exyL",
    "outputId": "80e798c7-060f-4ac7-d234-a3f2d28605d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longueur max des phrases: {'dev': 115, 'train': 271, 'test': 140}\n",
      "Rang max du target (en mots): {'dev': 96, 'train': 267, 'test': 115}\n",
      "SPLIT 18657 items into 16792 and 1865\n",
      "SPLIT 18657 items into 16792 and 1865\n",
      "SPLIT 18657 items into 16792 and 1865\n",
      "SPLIT 18657 items into 16792 and 1865\n",
      "dev : 2688 sentences, average lentgh=38.03\n",
      "train : 16792 sentences, average lentgh=38.99\n",
      "test : 3447 sentences, average lentgh=38.45\n",
      "val : 1865 sentences, average lentgh=38.97\n"
     ]
    }
   ],
   "source": [
    "gold_data_file = './asfalda_data_for_wsd/sequoiaftb.asfalda_1_3.gold.uniq.nofullant.txt'\n",
    "# les informations pour le split train / dev / test\n",
    "# tel qu'utilisé généralement pour ce corpus\n",
    "split_info_file = './asfalda_data_for_wsd/sequoiaftb_split_info'\n",
    "\n",
    "sentences, tg_wrks, tg_lemmas, label_strs = load_asfalda_data(gold_data_file,\n",
    "                                                              split_info_file, \n",
    "                                                              val_proportion=0.1)\n",
    "\n",
    "# récupération de tous les labels (= les frames) rencontrés\n",
    "all_labels_strs = []\n",
    "all_lemma_strs = []\n",
    "for p in sentences.keys():\n",
    "    all_labels_strs += label_strs[p]\n",
    "    all_lemma_strs += tg_lemmas[p]\n",
    "    avgl = sum([len(s) for s in sentences[p]])/len(sentences[p])\n",
    "    print(\"%s : %d sentences, average lentgh=%3.2f\" \n",
    "          %(p, len(sentences[p]), avgl))\n",
    "\n",
    "#@@ ATTENTION: ici vous codez tous les lemmes, y compris les lemmes du dev / test inconnus du train\n",
    "#   => cela donne une surestimation des performances utilisant les lemmes\n",
    "i2lemma = list(set(all_lemma_strs))\n",
    "lemma2i = {x:i for i,x in enumerate(i2lemma)}\n",
    "\n",
    "\n",
    "# id des labels (i.e. ici des frames)\n",
    "i2label = list(set(all_labels_strs))\n",
    "label2i = {x:i for i,x in enumerate(i2label)}\n",
    "# l'id du frame spécial \"Other_sense\"\n",
    "i_OTHER_SENSE = label2i['Other_sense']\n",
    "\n",
    "# séquence des ids de labels gold \n",
    "# pour chaque sous-corpus (clé = partie de corpus dev/train/test/val)\n",
    "labels = {}\n",
    "for p in label_strs.keys():\n",
    "    labels[p] = [label2i[x] for x in label_strs[p]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edb6wB9dfN2P",
    "outputId": "dd03efb8-b184-4650-bb4d-80b83b1a883a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_OTHER_SENSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIM0NNOQ9PHY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2Rgiftb-CQ5"
   },
   "source": [
    "### Baseline MFS (\"most frequent sense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "d2f7R9YD9_OH"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# calculer le sens le plus fréquent de chaque lemme-target\n",
    "# (le plus fréquent dans train+val)\n",
    "\n",
    "# et calculez la baseline MFS (\"most frequent sense\")\n",
    "# - sur le train+val\n",
    "# - sur le dev\n",
    "\n",
    "# TODO:\n",
    "# Etudiez les éléments de dev qui sont \"inconnus\" de train+val:\n",
    "# - les lemmes-target inconnus\n",
    "# - les frames inconnus\n",
    "# - les associations frame / lemme-target inconnus\n",
    "from collections import defaultdict\n",
    "def frequence(tg_lemmas,label_strs):\n",
    "  dict_lemmes = defaultdict(lambda: defaultdict(int))\n",
    "  dict_most_frequent = {}\n",
    "  for lemme, sens in zip(tg_lemmas,label_strs):\n",
    "      dict_lemmes[lemme][sens]+=1\n",
    "  for key, value in dict_lemmes.items():\n",
    "    max_item = max(value, key=lambda k: value[k])\n",
    "    dict_most_frequent[key] = max_item\n",
    "  return dict_lemmes, dict_most_frequent\n",
    "\n",
    "\n",
    "dict_lemmes_train, dict_most_frequent_train = frequence(tg_lemmas['train']+tg_lemmas['val'],label_strs['train']+label_strs['val'])\n",
    "dict_lemmes_val, dict_most_frequent_val = frequence(tg_lemmas['val'],label_strs['val'])\n",
    "dict_lemmes_dev, dict_most_frequent_dev = frequence(tg_lemmas['dev'],label_strs['dev'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "lUM5bg-4rDI3"
   },
   "outputs": [],
   "source": [
    "def baseline(dict_most_frequent, tg_lemmas,label_strs,train_or_dev = 'train+val'):\n",
    "  amount_correct = 0\n",
    "  for lemme, sens in zip(tg_lemmas,label_strs):\n",
    "    if sens == dict_most_frequent[lemme]:\n",
    "      amount_correct +=1\n",
    "  return 'Accuracy of baseline model on ' + train_or_dev + ' is:',amount_correct/len(tg_lemmas)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qq0DQCJsmUd",
    "outputId": "7cb446fd-062d-4767-e9cb-8ef33f8a6852"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Accuracy of baseline model on train+val is:', 81.39572278501367)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline(dict_most_frequent_train, tg_lemmas['train']+tg_lemmas['val'],label_strs['train']+label_strs['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p79BXMSq7R5N",
    "outputId": "f3489afc-a66b-4296-f984-38a014bc895c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Accuracy of baseline model on dev is:', 83.59375)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline(dict_most_frequent_dev, tg_lemmas['dev'],label_strs['dev'], 'dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0NvftzZs5TK"
   },
   "source": [
    "Now we need to find the lemmas, targets and the pairs of target + lemma that are not in train+val sets but are present in dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "QyvuxcYjtZBd"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "lemmes_inconnus = []\n",
    "sens_inconnus = []\n",
    "\n",
    "for lemma in tg_lemmas['dev']:\n",
    "  if lemma not in chain(tg_lemmas['train'], tg_lemmas['val']):\n",
    "    lemmes_inconnus.append(lemma)\n",
    "for sens in label_strs['dev']:\n",
    "  if sens not in chain(label_strs['train'], label_strs['val']):\n",
    "    sens_inconnus.append(sens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7m33grDnxGSw",
    "outputId": "6f3185ae-299c-4fce-f4c1-a4a7c29a984a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The number of unknown lemmas in dev is', 20)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'The number of unknown lemmas in dev is', len(set(lemmes_inconnus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2z3h5B_ZxsLk",
    "outputId": "2f35758a-0095-4a68-8513-9df0588de55d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The number of unknown sens in dev is', 0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'The number of unknown sens in dev is', len(set(sens_inconnus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "kLnlSS6tyZi2"
   },
   "outputs": [],
   "source": [
    "unknown_associations = []\n",
    "for lemma in dict_lemmes_dev.keys():\n",
    "  if lemma in dict_lemmes_train:\n",
    "    associations_possibles_train = dict_lemmes_train[lemma].keys()\n",
    "    associations_possibles_val = dict_lemmes_dev[lemma].keys()\n",
    "    for association_val in associations_possibles_val:\n",
    "      if association_val not in associations_possibles_train:\n",
    "         unknown_associations.append((lemma,association_val))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JbxiSKCp0X0z",
    "outputId": "032e8cb9-7013-45ea-c6b5-9c347268e153"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The number of unknown associations in dev is', 22)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'The number of unknown associations in dev is', len(set(unknown_associations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "dZDeGCyInz7w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'créer': 'Other_sense',\n",
       " 'voir_le_jour': 'Other_sense',\n",
       " 'autoriser': 'FR_Grant_permission-Permitting',\n",
       " 'connaître': 'Other_sense',\n",
       " 'inviter': 'FR_Attempt_suasion.to_do',\n",
       " 'conseil': 'Other_sense',\n",
       " 'pour': 'Other_sense',\n",
       " 'rappeler': 'FR_Statement-manner-noise',\n",
       " 'pourquoi': 'Causation',\n",
       " 'préciser': 'FR_Statement-manner-noise',\n",
       " 'parce_que': 'Causation',\n",
       " 'inciter': 'FR_Influence_of_event_on_cognizer-Subjective_influence',\n",
       " 'donc': 'Evidence',\n",
       " 'oublier': 'FR_Memory-Remembering_experience-Remembering_information',\n",
       " 'découvrir': 'FR_Becoming_aware',\n",
       " 'comme': 'Other_sense',\n",
       " 'ambition': 'FR_Purpose',\n",
       " 'apporter': 'Other_sense',\n",
       " 'témoignage': 'FR_Speak_on_topic',\n",
       " 'aussi': 'Other_sense',\n",
       " 'marquer': 'Other_sense',\n",
       " 'conduire': 'Other_sense',\n",
       " 'commenter': 'FR_Speak_on_topic',\n",
       " 'mener': 'Other_sense',\n",
       " 'contrôle': 'Other_sense',\n",
       " 'remerciement': 'Judgment_direct_address',\n",
       " 'création': 'Other_sense',\n",
       " 'nomination': 'Appointing',\n",
       " 'mobile': 'Other_sense',\n",
       " 'projet': 'FR_Purpose',\n",
       " 'conception': 'Other_sense',\n",
       " 'laisser': 'Other_sense',\n",
       " 'faire': 'Other_sense',\n",
       " 'afin_de': 'FR_Means_for_purpose',\n",
       " 'fruit': 'Other_sense',\n",
       " 'entendre': 'Other_sense',\n",
       " 'conter': 'FR_Statement-manner-noise',\n",
       " 'annoncer': 'FR_Statement-manner-noise',\n",
       " 'remettre': 'Other_sense',\n",
       " 'entraînement': 'Other_sense',\n",
       " 'apprendre': 'Other_sense',\n",
       " 'naître': 'Other_sense',\n",
       " 'compter': 'Other_sense',\n",
       " 'perdre': 'Other_sense',\n",
       " 'souvenir': 'FR_Memory-Remembering_experience-Remembering_information',\n",
       " 'prêt': 'Other_sense',\n",
       " 'encourager': 'FR_Attempt_suasion.to_do',\n",
       " \"jusqu'à\": 'Other_sense',\n",
       " 'chant': 'Other_sense',\n",
       " 'voir': 'Other_sense',\n",
       " 'devoir': 'Other_sense',\n",
       " 'sembler': 'FR_Awareness-Certainty-Opinion',\n",
       " 'origine': 'Other_sense',\n",
       " \"à_l'origine_de\": 'Causation',\n",
       " \"s'apercevoir\": 'FR_Becoming_aware',\n",
       " 'perdre_connaissance': 'Other_sense',\n",
       " 'noter': 'FR_Statement-manner-noise',\n",
       " 'témoin': 'Other_sense',\n",
       " 'alerter': 'FR_Telling',\n",
       " 'en_effet': 'Evidence',\n",
       " 'indiquer': 'FR_Statement-manner-noise',\n",
       " 'expliquer': 'FR_Statement-manner-noise',\n",
       " 'comprendre': 'Other_sense',\n",
       " 'réaliser': 'Other_sense',\n",
       " 'manifestation': 'Other_sense',\n",
       " 'aborder': 'FR_Speak_on_topic',\n",
       " 'compréhension': 'Other_sense',\n",
       " 'dialogue': 'FR_Chatting-Discussion',\n",
       " 'jouer': 'Other_sense',\n",
       " 'applaudissement': 'FR_Judgment_communication',\n",
       " \"s'exprimer\": 'FR_Speak_on_topic',\n",
       " 'classement': 'Other_sense',\n",
       " 'alors': 'Other_sense',\n",
       " 'connaissance': 'Other_sense',\n",
       " 'affecter': 'FR_Contingency-Objective_influence',\n",
       " 'sinon': 'Other_sense',\n",
       " 'rendre_son_tablier': 'Other_sense',\n",
       " 'soutenir': 'Other_sense',\n",
       " 'querelle': 'Quarreling',\n",
       " 'faire_partie': 'Other_sense',\n",
       " 'empêcher': 'Preventing',\n",
       " 'demander': 'FR_Request',\n",
       " 'régler': 'Other_sense',\n",
       " 'faire_son_apparition': 'Other_sense',\n",
       " 'conscience': 'FR_Awareness-Certainty-Opinion',\n",
       " 'contre': 'Other_sense',\n",
       " 'sûr': 'FR_Awareness-Certainty-Opinion',\n",
       " 'commencer': 'Other_sense',\n",
       " 'faire_sentir': 'Other_sense',\n",
       " 'interpeller': 'Other_sense',\n",
       " 'appel_téléphonique': 'FR_Contacting',\n",
       " \"représentant_de_l'ordre\": 'Other_sense',\n",
       " 'assurer': 'Other_sense',\n",
       " 'répondre': 'Response',\n",
       " 'savoir': 'FR_Awareness-Certainty-Opinion',\n",
       " 'solde': 'Other_sense',\n",
       " 'grâce_à': 'Causation',\n",
       " 'marché': 'FR_Having_commercial_agreement',\n",
       " 'vente': 'Commerce_sell',\n",
       " 'discuter': 'FR_Chatting-Discussion',\n",
       " 'frais': 'FR_Spending',\n",
       " 'décider': 'FR_Deciding',\n",
       " 'envisager': 'FR_Purpose',\n",
       " 'soutien': 'Other_sense',\n",
       " 'permettre': 'Make_possible_to_do',\n",
       " 'lancer': 'FR_Cause_to_start-Launch_process',\n",
       " 'car': 'Evidence',\n",
       " 'raison': 'FR_Reason',\n",
       " 'refuser': 'FR_Agree_or_refuse_to_act',\n",
       " 'protester': 'Complaining',\n",
       " 'verser': 'FR_Giving_money',\n",
       " 'certain': 'Other_sense',\n",
       " 'condition': 'Other_sense',\n",
       " 'attirer': 'Other_sense',\n",
       " 'trouver': 'Other_sense',\n",
       " 'reconnaître': 'FR_Statement-manner-noise',\n",
       " 'subir': 'FR_Support_verb',\n",
       " 'écrire': 'FR_Statement-manner-noise',\n",
       " 'dire': 'FR_Statement-manner-noise',\n",
       " 'reprise': 'Other_sense',\n",
       " 'découler': 'Causation',\n",
       " 'puisque': 'Evidence',\n",
       " 'montrer': 'Evidence',\n",
       " 'responsable': 'Other_sense',\n",
       " 'conseiller': 'Other_sense',\n",
       " 'afin_que': 'FR_Means_for_purpose',\n",
       " 'point_de_vue': 'Other_sense',\n",
       " 'critique': 'Other_sense',\n",
       " 'pousser': 'Other_sense',\n",
       " 'contacter': 'FR_Contacting',\n",
       " 'ouvrir': 'Other_sense',\n",
       " 'confier': 'Other_sense',\n",
       " 'réaction': 'Response',\n",
       " 'conséquence': 'Causation',\n",
       " 'réponse': 'Communication_response',\n",
       " 'plaider': 'FR_Attempt_suasion.legitimacy',\n",
       " 'reprocher': 'Judgment_direct_address',\n",
       " 'dénoncer': 'FR_Judgment_communication',\n",
       " 'entamer': 'FR_Cause_to_start-Launch_process',\n",
       " 'signaler': 'FR_Statement-manner-noise',\n",
       " 'déclarer': 'FR_Statement-manner-noise',\n",
       " 'prévoir': 'Other_sense',\n",
       " 'hommage': 'FR_Judgment_communication',\n",
       " 'effet': 'Causation',\n",
       " 'menacer': 'Other_sense',\n",
       " 'disposer': 'Other_sense',\n",
       " 'fonction': 'Other_sense',\n",
       " 'alerte': 'FR_Telling',\n",
       " 'appel': 'Other_sense',\n",
       " 'prévenir': 'FR_Telling',\n",
       " 'suite': 'Other_sense',\n",
       " 'reprendre': 'Other_sense',\n",
       " 'preuve_que': 'Evidence',\n",
       " 'cotisation': 'Commerce_pay',\n",
       " 'idée': 'Other_sense',\n",
       " 'appeler': 'Referring_by_name',\n",
       " 'raconter': 'FR_Statement-manner-noise',\n",
       " 'faire_part': 'FR_Telling',\n",
       " 'conclusion': 'Coming_to_believe',\n",
       " 'favorable': 'Other_sense',\n",
       " 'prendre_connaissance': 'Other_sense',\n",
       " 'pour_que': 'Other_sense',\n",
       " 'décision': 'FR_Deciding',\n",
       " 'sachant_que': 'Other_sense',\n",
       " \"faire_l'objet\": 'Other_sense',\n",
       " 'se_prononcer': 'FR_Expressing_side',\n",
       " 'à_raison_de': 'Other_sense',\n",
       " 'porter': 'Other_sense',\n",
       " 'accord': 'Be_in_agreement_on_action',\n",
       " 'tenir': 'Other_sense',\n",
       " 'faire_confiance': 'Other_sense',\n",
       " 'atteindre': 'Other_sense',\n",
       " 'remercier': 'Judgment_direct_address',\n",
       " 'constat': 'FR_Becoming_aware',\n",
       " 'forcer': 'FR_Reason',\n",
       " 'motif': 'FR_Reason',\n",
       " 'contraindre': 'FR_Reason',\n",
       " 'entraîner': 'Causation',\n",
       " 'questionner': 'Questioning',\n",
       " 'se_souvenir': 'FR_Memory-Remembering_experience-Remembering_information',\n",
       " 'apprécier': 'Judgment',\n",
       " 'faire_remarquer': 'FR_Telling',\n",
       " 'rapport': 'FR_Statement-manner-noise',\n",
       " 'résumé': 'Summarizing',\n",
       " 'intention': 'FR_Purpose',\n",
       " 'discussion': 'FR_Chatting-Discussion',\n",
       " 'dépendre': 'FR_Contingency-Objective_influence',\n",
       " 'produire': 'Other_sense',\n",
       " 'impliquer': 'Other_sense',\n",
       " 'observer': 'FR_Becoming_aware',\n",
       " 'approuver': 'Ratification',\n",
       " 'estimer': 'FR_Awareness-Certainty-Opinion',\n",
       " 'conclure': 'FR_Support_verb',\n",
       " 'recommander': 'FR_Attempt_suasion.to_do',\n",
       " 'autorisation': 'FR_Grant_permission-Permitting',\n",
       " 'indication': 'Other_sense',\n",
       " 'se_justifier': 'Other_sense',\n",
       " 'par_conséquent': 'Causation',\n",
       " 'en_raison_de': 'Causation',\n",
       " 'produit': 'Other_sense',\n",
       " 'contrôler': 'Other_sense',\n",
       " 'dès_lors': 'Evidence',\n",
       " 'convenir': 'Other_sense',\n",
       " 'symptôme': 'Evidence',\n",
       " 'soupçonner': 'FR_Awareness-Certainty-Opinion',\n",
       " 'rapporter': 'FR_Statement-manner-noise',\n",
       " 'faire_face': 'Other_sense',\n",
       " 'informer': 'FR_Telling',\n",
       " 'signe': 'Other_sense',\n",
       " 'résultat': 'Other_sense',\n",
       " 'penser': 'FR_Awareness-Certainty-Opinion',\n",
       " 'du_fait_de': 'Causation',\n",
       " \"s'attendre\": 'FR_Expectation',\n",
       " 'selon': 'FR_Attributed_information',\n",
       " 'constater': 'FR_Becoming_aware',\n",
       " 'perte': 'Earnings_and_losses',\n",
       " 'mentionner': 'FR_Statement-manner-noise',\n",
       " 'résumer': 'Summarizing',\n",
       " 'inconnu': 'FR_Awareness-Certainty-Opinion',\n",
       " 'détecter': 'FR_Becoming_aware',\n",
       " 'dans_la_mesure_où': 'Evidence',\n",
       " 'ainsi': 'Other_sense',\n",
       " 'induire': 'Causation',\n",
       " 'prouver': 'FR_Proving',\n",
       " 'recommandation': 'FR_Attempt_suasion.to_do',\n",
       " 'exiger': 'FR_Request',\n",
       " 'source': 'Other_sense',\n",
       " 'distribution': 'Carry_goods',\n",
       " 'résulter': 'Causation',\n",
       " 'répéter': 'Other_sense',\n",
       " 'révéler': 'Evidence',\n",
       " 'démontrer': 'FR_Proving',\n",
       " 'relever': 'Other_sense',\n",
       " 'commercialiser': 'Carry_goods',\n",
       " 'ajouter': 'FR_Statement-manner-noise',\n",
       " \"jusqu'à_ce_que\": 'Other_sense',\n",
       " 'prélever': 'FR_Getting_money',\n",
       " 'étiquetage': 'Other_sense',\n",
       " 'vue': 'Other_sense',\n",
       " 'justification': 'FR_Justifying',\n",
       " 'mention': 'FR_Statement-manner-noise',\n",
       " 'doute': 'Other_sense',\n",
       " 'remarquer': 'FR_Becoming_aware',\n",
       " 'déterminer': 'Other_sense',\n",
       " 'dû': 'Causation',\n",
       " 'incidence': 'Other_sense',\n",
       " 'aboutir': 'Causation',\n",
       " 'requérir': 'Other_sense',\n",
       " 'provoquer': 'Causation',\n",
       " 'symptomatique': 'Other_sense',\n",
       " 'facteur': 'Causation',\n",
       " 'éviter': 'Preventing',\n",
       " 'classer': 'Categorization',\n",
       " 'commercialisation': 'Carry_goods',\n",
       " 'incertain': 'Other_sense',\n",
       " 'prévention': 'Preventing',\n",
       " 'confirmer': 'FR_Expressing_truth_of_proposition',\n",
       " 'cause': 'Other_sense',\n",
       " 'refléter': 'Evidence',\n",
       " 'convertir': 'Other_sense',\n",
       " 'dites': 'FR_Statement-manner-noise',\n",
       " 'menace': 'Other_sense',\n",
       " 'fonder': 'Other_sense',\n",
       " 'exposé': 'FR_Speak_on_topic',\n",
       " 'à_des_fins_de': 'FR_Means_for_purpose',\n",
       " 'objectif': 'FR_Purpose',\n",
       " 'interdire': 'FR_Grant_permission-Permitting',\n",
       " 'rembourser': 'Repayment',\n",
       " 'douter': 'FR_Awareness-Certainty-Opinion',\n",
       " 'considérer': 'FR_Awareness-Certainty-Opinion',\n",
       " 'allusion': 'FR_Speak_on_topic',\n",
       " 'débat': 'FR_Chatting-Discussion',\n",
       " 'fondement': 'Other_sense',\n",
       " 'résolution': 'Other_sense',\n",
       " \"voir_d'un_bon_oeil\": 'FR_Being_in_favor_of',\n",
       " 'rejeter': 'FR_Taking_sides',\n",
       " 'condamnation': 'Other_sense',\n",
       " 'interpréter': 'FR_Awareness-Certainty-Opinion',\n",
       " 'opinion': 'Other_sense',\n",
       " 'opposition': 'Other_sense',\n",
       " 'partisan': 'FR_Being_in_favor_of',\n",
       " 'croire': 'FR_Awareness-Certainty-Opinion',\n",
       " 'faire_preuve': 'Other_sense',\n",
       " 'opposé': 'Other_sense',\n",
       " 'se_rappeler': 'FR_Memory-Remembering_experience-Remembering_information',\n",
       " 'de_ce_fait': 'Causation',\n",
       " 'parler': 'FR_Speak_on_topic',\n",
       " 'remarque': 'FR_Statement-manner-noise',\n",
       " 'avis': 'FR_Awareness-Certainty-Opinion',\n",
       " 'regretter': 'FR_Taking_sides',\n",
       " 'souligner': 'Convey_importance',\n",
       " 'réagir': 'Response',\n",
       " 'condamner': 'Other_sense',\n",
       " 'faire_un_geste': 'Other_sense',\n",
       " \"d'accord\": 'Be_in_agreement_on_action',\n",
       " 'obliger': 'FR_Reason',\n",
       " \"c'est_pourquoi\": 'Causation',\n",
       " 'diviser': 'Other_sense',\n",
       " 'promettre': 'FR_Commitment.to_do',\n",
       " 'admettre': 'FR_Awareness-Certainty-Opinion',\n",
       " 'débattre': 'FR_Chatting-Discussion',\n",
       " 'étant_donné_que': 'Evidence',\n",
       " 'toucher': 'FR_Contingency-Objective_influence',\n",
       " 'argument': 'FR_Attempt_supported_suasion',\n",
       " 'argumenter': 'FR_Attempt_supported_suasion',\n",
       " 'concéder': 'FR_Statement-manner-noise',\n",
       " 'accepter': 'FR_Taking_sides',\n",
       " 'affirmation': 'FR_Statement-manner-noise',\n",
       " 'demande': 'Other_sense',\n",
       " 'déclaration': 'FR_Statement-manner-noise',\n",
       " 'concevoir': 'Other_sense',\n",
       " 'règlement': 'Other_sense',\n",
       " 'en_vue_de': 'FR_Means_for_purpose',\n",
       " 'en_conséquence': 'Causation',\n",
       " 'soulever': 'FR_Encoding',\n",
       " 'divergence': 'Be_in_agreement_on_assessment',\n",
       " 'justifier': 'FR_Justifying',\n",
       " \"d'avis\": 'FR_Awareness-Certainty-Opinion',\n",
       " 'faire_défaut': 'Other_sense',\n",
       " 'preuve': 'Evidence',\n",
       " 'accuser': 'FR_Judgment_communication',\n",
       " 'faute_de': 'Causation',\n",
       " 'propos': 'FR_Speak_on_topic',\n",
       " 'discours': 'FR_Speak_on_topic',\n",
       " 'en_vertu_de': 'FR_Reason',\n",
       " 'à_proprement_parler': 'Other_sense',\n",
       " 'contradiction': 'Other_sense',\n",
       " 'vu_que': 'Evidence',\n",
       " 'démentir': 'FR_Expressing_truth_of_proposition',\n",
       " 'défense': 'Other_sense',\n",
       " 'ordre_du_jour': 'Other_sense',\n",
       " 'faire_état': 'FR_Statement-manner-noise',\n",
       " 'exprimer': 'FR_Encoding',\n",
       " 'merci': 'Judgment_direct_address',\n",
       " 'exposer': 'Other_sense',\n",
       " 'refus': 'FR_Agree_or_refuse_to_act',\n",
       " 'défendre': 'Other_sense',\n",
       " 'payer': 'Commerce_pay',\n",
       " 'complimenter': 'Judgment_direct_address',\n",
       " 'à_son_avis': 'FR_Awareness-Certainty-Opinion',\n",
       " 'citer': 'Adducing',\n",
       " 'débourser': 'FR_Spending',\n",
       " 'exigence': 'FR_Request',\n",
       " 'coûter': 'FR_Spending',\n",
       " 'engagement': 'FR_Commitment.to_do',\n",
       " 'commerce': 'FR_Commerce_scenario',\n",
       " 'convaincre': 'FR_Cognizer_affecting.veracity',\n",
       " 'acquisition': 'Commerce_buy',\n",
       " 'assurance': 'Other_sense',\n",
       " 'féliciter': 'Judgment_direct_address',\n",
       " 'effectivement': 'Other_sense',\n",
       " \"s'adresser\": 'FR_Speak_on_topic',\n",
       " 'faire_savoir': 'FR_Telling',\n",
       " 'résoudre': 'Other_sense',\n",
       " 'promulguer': 'Ratification',\n",
       " 'interdiction': 'FR_Grant_permission-Permitting',\n",
       " 'se_faire_attendre': 'Other_sense',\n",
       " 'attente': 'Other_sense',\n",
       " 'divulguer': 'Reveal_secret',\n",
       " 'importer': 'Importing',\n",
       " 'lancement': 'Other_sense',\n",
       " 'coût': 'FR_Spending',\n",
       " 'supposer': 'Other_sense',\n",
       " 'initier': 'FR_Cause_to_start-Launch_process',\n",
       " 'évoquer': 'FR_Speak_on_topic',\n",
       " 'à_cause_de': 'Causation',\n",
       " 'rendre': 'Causation',\n",
       " 'paiement': 'Commerce_pay',\n",
       " 'imputer': 'FR_Attributing_cause',\n",
       " 'se_flatter': 'Bragging',\n",
       " 'sceptique': 'FR_Awareness-Certainty-Opinion',\n",
       " 'requête': 'FR_Request',\n",
       " 'commentaire': 'FR_Speak_on_topic',\n",
       " 'témoigner': 'Evidence',\n",
       " 'permission': 'FR_Grant_permission-Permitting',\n",
       " 'préconiser': 'FR_Attempt_suasion.to_do',\n",
       " 'expression': 'Other_sense',\n",
       " 'nier': 'FR_Expressing_truth_of_proposition',\n",
       " 'exportation': 'Exporting',\n",
       " 'ordre': 'Other_sense',\n",
       " 'paraître': 'FR_Awareness-Certainty-Opinion',\n",
       " 'polémique': 'Quarreling',\n",
       " 'jugement': 'Other_sense',\n",
       " 'acquitter': 'Commerce_pay',\n",
       " 'protestation': 'Complaining',\n",
       " 'juger': 'FR_Awareness-Certainty-Opinion',\n",
       " \"s'inspirer\": 'FR_Influence_of_event_on_cognizer-Subjective_influence',\n",
       " \"s'appeler\": 'FR_Being_named',\n",
       " 'communication': 'Other_sense',\n",
       " \"d'après\": 'FR_Attributed_information',\n",
       " 'accusation': 'FR_Judgment_communication',\n",
       " 'mépris': 'Judgment',\n",
       " 'infirmer': 'Evidence',\n",
       " 'vendre': 'Commerce_sell',\n",
       " 'dévoiler': 'Reveal_secret',\n",
       " 'dans_le_but_de': 'FR_Means_for_purpose',\n",
       " 'discréditer': 'FR_Judgment_communication',\n",
       " 'contester': 'FR_Taking_sides',\n",
       " 'affirmer': 'FR_Statement-manner-noise',\n",
       " 'vu': 'Evidence',\n",
       " 'ordonner': 'FR_Request',\n",
       " 'sur_la_demande_de': 'Response',\n",
       " 'rémunérer': 'Commerce_buy',\n",
       " 'présomption': 'FR_Awareness-Certainty-Opinion',\n",
       " 'prononcer': 'FR_Expressing_decision',\n",
       " 'valider': 'Ratification',\n",
       " 'suspecter': 'FR_Awareness-Certainty-Opinion',\n",
       " 'soupçon': 'FR_Awareness-Certainty-Opinion',\n",
       " 'au_motif_que': 'FR_Reason',\n",
       " 'notifier': 'FR_Telling',\n",
       " 'révélation': 'Reveal_secret',\n",
       " 'réclamer': 'FR_Request',\n",
       " 'marque': 'Other_sense',\n",
       " 'mépriser': 'Judgment',\n",
       " 'faire_appel': 'Other_sense',\n",
       " 'engager': 'Other_sense',\n",
       " 'désapprouver': 'Judgment',\n",
       " 'rallier': 'FR_Being_in_favor_of',\n",
       " 'nommer': 'Appointing',\n",
       " 'déclenchement': 'FR_Cause_to_start-Launch_process',\n",
       " 'persuader': 'FR_Awareness-Certainty-Opinion',\n",
       " 'douteux': 'Other_sense',\n",
       " 'transaction': 'FR_Commercial_transaction',\n",
       " 'objection': 'Communication_response',\n",
       " 'contrat': 'FR_Having_commercial_agreement',\n",
       " 'acheter': 'Commerce_buy',\n",
       " 'distribuer': 'Other_sense',\n",
       " 'rémunération': 'Commerce_pay',\n",
       " 'consentir': 'FR_Agree_or_refuse_to_act_in_favor_of',\n",
       " 'motiver': 'FR_Reason',\n",
       " \"s'élever\": 'Other_sense',\n",
       " 'opposer': 'Other_sense',\n",
       " 'susciter': 'FR_Reason',\n",
       " 'certitude': 'FR_Awareness-Certainty-Opinion',\n",
       " 'ignorance': 'FR_Awareness-Certainty-Opinion',\n",
       " 'censé': 'Other_sense',\n",
       " 'désaccord': 'Be_in_agreement_on_assessment',\n",
       " 'entrevoir': 'FR_Becoming_aware',\n",
       " 'oubli': 'FR_Memory-Remembering_experience-Remembering_information',\n",
       " 'remboursement': 'Repayment',\n",
       " 'découverte': 'FR_Becoming_aware',\n",
       " 'versement': 'FR_Giving_money',\n",
       " 'déplorer': 'FR_Taking_sides',\n",
       " 'interroger': 'Questioning',\n",
       " 'inattendu': 'FR_Expectation',\n",
       " 'au_courant': 'FR_Awareness-Certainty-Opinion',\n",
       " 'influence': 'FR_Influence_of_event_on_cognizer-Subjective_influence',\n",
       " 'malédiction': 'FR_Judgment_communication_no_reason',\n",
       " 'rachat': 'Commerce_buy',\n",
       " 'racheter': 'Commerce_buy',\n",
       " 'citation': 'FR_Quoting',\n",
       " 'solliciter': 'FR_Request',\n",
       " 'à_la_demande_de': 'Response',\n",
       " 'en_désaccord': 'FR_Being_in_favor_of',\n",
       " 'débuter': 'Other_sense',\n",
       " 'facturation': 'Commerce_collect',\n",
       " 'foi': 'Other_sense',\n",
       " 'reverser': 'FR_Giving_money',\n",
       " 'percevoir': 'FR_Getting_money',\n",
       " 'conversation': 'FR_Chatting-Discussion',\n",
       " 'incertitude': 'FR_Awareness-Certainty-Opinion',\n",
       " 'coup_de_fil': 'FR_Contacting',\n",
       " 'achat': 'Commerce_buy',\n",
       " 'paie': 'Commerce_pay',\n",
       " 'indice': 'Other_sense',\n",
       " 'attribuer': 'Other_sense',\n",
       " 'confession': 'Reveal_secret',\n",
       " \"s'opposer\": 'FR_Taking_sides',\n",
       " 'renoncer': 'FR_Renunciation',\n",
       " 'dicter': 'Other_sense',\n",
       " 'influencer': 'FR_Contingency-Objective_influence',\n",
       " 'compte-rendu': 'Summarizing',\n",
       " 'acquérir': 'Commerce_buy',\n",
       " 'aboutissement': 'Other_sense',\n",
       " 'manifester': 'Other_sense',\n",
       " 'détailler': 'FR_Speak_on_topic',\n",
       " 'imaginer': 'Other_sense',\n",
       " 'sur_ordre_de': 'Response',\n",
       " 'avouer': 'Reveal_secret',\n",
       " 'dialoguer': 'FR_Chatting-Discussion',\n",
       " 'rajouter': 'Other_sense',\n",
       " 'explication': 'FR_Speak_on_topic',\n",
       " 'décourager': 'FR_Influence_of_event_on_cognizer-Subjective_influence',\n",
       " 'se_rendre_compte': 'FR_Becoming_aware',\n",
       " 'importation': 'Importing',\n",
       " 'céder': 'Commerce_sell',\n",
       " 'du_coup': 'Causation',\n",
       " 'se_résoudre': 'FR_Deciding',\n",
       " 'renvoyer_la_balle': 'Other_sense',\n",
       " 'amener': 'FR_Reason',\n",
       " 'prétendre': 'Other_sense',\n",
       " 'répercussion': 'Causation',\n",
       " 'dépense': 'FR_Spending',\n",
       " 'assuré': 'Other_sense',\n",
       " \"c'est_-_à_-_dire\": 'Other_sense',\n",
       " 'admiration': 'Judgment',\n",
       " 'conscient': 'FR_Awareness-Certainty-Opinion',\n",
       " 'à_preuve': 'Evidence',\n",
       " 'se_targuer': 'Bragging',\n",
       " \"s'allier\": 'Make_agreement_on_action',\n",
       " 'décrier': 'FR_Judgment_communication',\n",
       " 'au_point_que': 'Causation',\n",
       " 'tolérer': 'FR_Being_in_favor_of',\n",
       " 'prélèvement': 'FR_Getting_money',\n",
       " 'prendre_sa_source': 'Causation',\n",
       " 'privatisation': 'Commerce_sell',\n",
       " 'conséquent': 'Other_sense',\n",
       " 'encaisser': 'FR_Getting_money',\n",
       " 'impulsion': 'Other_sense',\n",
       " 'applaudir': 'FR_Judgment_communication',\n",
       " 'conviction': 'FR_Awareness-Certainty-Opinion',\n",
       " 'porter_-_parole': 'Other_sense',\n",
       " 'baptiser': 'Name_conferral',\n",
       " 'tabler': 'FR_Reliance_on_expectation',\n",
       " 'stimuler': 'FR_Contingency-Objective_influence',\n",
       " 'reproche': 'Judgment_direct_address',\n",
       " 'renchérir': 'Other_sense',\n",
       " 'renoncement': 'FR_Renunciation',\n",
       " 'critiquer': 'FR_Judgment_communication',\n",
       " 'but': 'FR_Purpose',\n",
       " 'ignorer': 'Other_sense',\n",
       " 'vanter': 'FR_Judgment_communication',\n",
       " 'gagner': 'Other_sense',\n",
       " 'inspirer': 'Other_sense',\n",
       " 'trancher': 'FR_Expressing_side',\n",
       " 'de_fait': 'Evidence',\n",
       " 'reconnaissance': 'FR_Statement-manner-noise',\n",
       " 'se_défendre': 'FR_Expressing_truth_of_proposition',\n",
       " 'atténuer': 'FR_Contingency-Objective_influence',\n",
       " 'amorcer': 'Other_sense',\n",
       " 'moteur': 'Other_sense',\n",
       " 'classification': 'Categorization',\n",
       " 'motivation': 'FR_Influence_of_event_on_cognizer-Subjective_influence',\n",
       " 'être_ce_à_dire': 'Other_sense',\n",
       " 'presser': 'Other_sense',\n",
       " 'engendrer': 'FR_Cause_to_start-Launch_process',\n",
       " 'avancer': 'FR_Statement-manner-noise',\n",
       " 'alliance': 'Be_in_agreement_on_action',\n",
       " 'déterminant': 'FR_Contingency-Objective_influence',\n",
       " 'escompter': 'FR_Reliance_on_expectation',\n",
       " 'au_vu_de': 'Evidence',\n",
       " 'avoir_pour_nom': 'FR_Being_named',\n",
       " 'indicateur': 'Other_sense',\n",
       " 'générer': 'Other_sense',\n",
       " 'conversion': 'Other_sense',\n",
       " \"tomber_d'accord\": 'FR_Make_agreement_on_assessment',\n",
       " 'confirmation': 'Ratification',\n",
       " 'à_la_suite_de': 'Causation',\n",
       " \"s'engager\": 'FR_Commitment.to_do',\n",
       " 'brader': 'Commerce_sell',\n",
       " 'souscrire': 'Other_sense',\n",
       " 'formulation': 'FR_Encoding',\n",
       " \"voir_d'un_mauvais_oeil\": 'FR_Being_in_favor_of',\n",
       " 'contact': 'FR_Contacting',\n",
       " 'pourparler': 'FR_Chatting-Discussion',\n",
       " 'pour_preuve': 'Evidence',\n",
       " 'de_façon_à': 'FR_Means_for_purpose',\n",
       " 'laisser_entendre': 'FR_Encoding',\n",
       " 'contrer': 'Response',\n",
       " 'congratuler': 'Judgment_direct_address',\n",
       " 'fustiger': 'FR_Judgment_communication',\n",
       " 'cotiser': 'Commerce_pay',\n",
       " 'conditionner': 'Other_sense',\n",
       " 'déterminé': 'Other_sense',\n",
       " 'prévision': 'FR_Expectation',\n",
       " 'faire_ses_preuves': 'Other_sense',\n",
       " 'donner_raison': 'Other_sense',\n",
       " 'faire_suite': 'Other_sense',\n",
       " 'réticent': 'Willingness',\n",
       " 'se_rallier': 'FR_Being_in_favor_of',\n",
       " 'infliger': 'Other_sense',\n",
       " 'enjoindre': 'FR_Request',\n",
       " 'écrit': 'Text_creation',\n",
       " 'repousser': 'Other_sense',\n",
       " 'accréditer': 'Evidence',\n",
       " 'impact': 'Causation',\n",
       " 'déclencher': 'FR_Cause_to_start-Launch_process',\n",
       " 'déduire': 'Other_sense',\n",
       " 'gain': 'Other_sense',\n",
       " 'pour_cause_de': 'Causation',\n",
       " 'parier': 'FR_Reliance_on_expectation',\n",
       " 'sentir': 'FR_Awareness-Certainty-Opinion',\n",
       " 'faire_observer': 'FR_Telling',\n",
       " \"mot_d'ordre\": 'Other_sense',\n",
       " \"c'est_à_-_dire\": 'Other_sense',\n",
       " 'imprévisible': 'FR_Expectation',\n",
       " 'écouler': 'Commerce_sell',\n",
       " 'contrainte': 'Other_sense',\n",
       " 'croyance': 'FR_Awareness-Certainty-Opinion',\n",
       " 'élogieux': 'FR_Judgment_communication',\n",
       " 'paye': 'Other_sense',\n",
       " 'écriture': 'Other_sense',\n",
       " 'impression': 'FR_Awareness-Certainty-Opinion',\n",
       " 'exporter': 'Exporting',\n",
       " 'sentiment': 'FR_Awareness-Certainty-Opinion',\n",
       " 'à_ce_point_que': 'Causation',\n",
       " 'anathème': 'FR_Judgment_communication',\n",
       " \"s'entendre\": 'Make_agreement_on_action',\n",
       " 'adhésion': 'FR_Being_in_favor_of',\n",
       " 'entériner': 'Ratification',\n",
       " 'prendre_position': 'Other_sense',\n",
       " 'perception': 'FR_Awareness-Certainty-Opinion',\n",
       " 'décliner': 'Other_sense',\n",
       " 'revendre': 'Commerce_sell',\n",
       " 'songer': 'Other_sense',\n",
       " 'adhérer': 'Other_sense',\n",
       " \"de_l'ordre_de\": 'Other_sense',\n",
       " 'issue': 'Causation',\n",
       " 'dès_lors_que': 'Other_sense',\n",
       " 'se_faire_prier': 'Other_sense',\n",
       " 'illusion': 'FR_Awareness-Certainty-Opinion',\n",
       " \"s'accorder\": 'Be_in_agreement_on_assessment',\n",
       " 'arrangement': 'Be_in_agreement_on_action',\n",
       " 'négoce': 'FR_Commerce_scenario',\n",
       " 'éprouver': 'Other_sense',\n",
       " 'attester': 'Evidence',\n",
       " 'nationaliser': 'Commerce_buy',\n",
       " 'anticipation': 'Other_sense',\n",
       " 'solder': 'Other_sense',\n",
       " 'avertir': 'FR_Telling',\n",
       " 'pressentir': 'Other_sense',\n",
       " 'vouloir_dire': 'Other_sense',\n",
       " 'écoulement': 'Commerce_sell',\n",
       " 'exalter': 'FR_Judgment_communication',\n",
       " 'serment': 'FR_Commitment.to_do',\n",
       " 'projeter': 'FR_Purpose',\n",
       " 'alléguer': 'Adducing',\n",
       " 'prôner': 'FR_Attempt_suasion.legitimacy',\n",
       " 'alimenter': 'Other_sense',\n",
       " 'au_fait': 'FR_Awareness-Certainty-Opinion',\n",
       " 'accentuer': 'FR_Contingency-Objective_influence',\n",
       " 'imputable': 'FR_Attributing_cause',\n",
       " 'à_défaut_de': 'Other_sense',\n",
       " 'plaidoyer': 'FR_Attempt_supported_suasion',\n",
       " 'à_vrai_dire': 'Other_sense',\n",
       " 'cession': 'Commerce_sell',\n",
       " 'réplique': 'FR_Statement-manner-noise',\n",
       " 'rétorquer': 'Communication_response',\n",
       " 'générateur': 'FR_Cause_to_start-Launch_process',\n",
       " 'dans_ce_but': 'FR_Means_for_purpose',\n",
       " 'précision': 'FR_Statement-manner-noise',\n",
       " 'animer': 'Other_sense',\n",
       " 'surcoût': 'FR_Spending',\n",
       " 'se_féliciter': 'FR_Taking_sides',\n",
       " 'mener_à_bien': 'Other_sense',\n",
       " 'louer': 'Renting_out',\n",
       " 'pour_ainsi_dire': 'Other_sense',\n",
       " 'prédire': 'Predicting',\n",
       " 'et_pour_cause': 'Causation',\n",
       " 'promesse': 'FR_Commitment.to_do',\n",
       " \"s'exclamer\": 'FR_Statement-manner-noise',\n",
       " 'dépenser': 'FR_Spending',\n",
       " 'causer': 'Causation',\n",
       " 'trouver_sa_source': 'Causation',\n",
       " 'illustrer': 'Evidence',\n",
       " 'donner_naissance': 'Other_sense',\n",
       " 'dessein': 'Other_sense',\n",
       " 'interrogation': 'Questioning',\n",
       " 'dédaigner': 'Judgment',\n",
       " 'rendre_compte': 'Other_sense',\n",
       " 'rejet': 'FR_Taking_sides',\n",
       " 'suggestion': 'FR_Attempt_suasion.to_do',\n",
       " 'de_sorte_que': 'Causation',\n",
       " 'prière': 'Other_sense',\n",
       " 'revente': 'Commerce_sell',\n",
       " 'en_coûter': 'FR_Spending',\n",
       " 'hypothèse': 'FR_Awareness-Certainty-Opinion',\n",
       " 'bradage': 'Commerce_sell',\n",
       " \"d'où\": 'Causation',\n",
       " 'gronder': 'Other_sense',\n",
       " \"s'insurger\": 'FR_Taking_sides',\n",
       " 'porter_le_à': 'Other_sense',\n",
       " 'mettre_sur_le_compte_de': 'FR_Attributing_cause',\n",
       " 'se_résumer': 'Other_sense',\n",
       " 'arguer': 'FR_Justifying',\n",
       " 'jurer': 'FR_Commitment.to_do',\n",
       " 'location': 'Renting_out',\n",
       " 'au_point_de': 'Causation',\n",
       " 'anticiper': 'FR_Expectation',\n",
       " 'tant_et_si_bien_que': 'Causation',\n",
       " 'se_convertir': 'FR_Being_in_favor_of',\n",
       " 'louange': 'FR_Judgment_communication',\n",
       " \"sous_l'effet_de\": 'Causation',\n",
       " 'allumer': 'Other_sense',\n",
       " 'avaliser': 'Ratification',\n",
       " 'communiquer': 'FR_Telling',\n",
       " 'annonce': 'FR_Statement-manner-noise',\n",
       " 'dits': 'Other_sense',\n",
       " \"raison_d'être\": 'Other_sense',\n",
       " 'bonifier': 'Commerce_buy',\n",
       " 'de_manière_à': 'FR_Means_for_purpose',\n",
       " 'à_cet_effet': 'FR_Means_for_purpose',\n",
       " 'désavouer': 'FR_Expressing_truth_of_proposition',\n",
       " 'ménager': 'Other_sense',\n",
       " 'prise_de_conscience': 'FR_Becoming_aware',\n",
       " 'incitation': 'FR_Influence_of_event_on_cognizer-Subjective_influence',\n",
       " 'acceptation': 'FR_Taking_sides',\n",
       " 'prendre_la_parole': 'FR_Speak_on_topic',\n",
       " 'à_sa_demande': 'Response',\n",
       " 'démarrer': 'Other_sense',\n",
       " \"donneur_d'ordre\": 'Other_sense',\n",
       " 'donneur_ordre': 'Other_sense',\n",
       " 'ce_être_-_à_-_dire': 'Other_sense',\n",
       " \"s'indigner\": 'FR_Taking_sides',\n",
       " 'déni': 'Other_sense',\n",
       " 'au_mépris_de': 'Other_sense',\n",
       " 'contredire': 'Evidence',\n",
       " 'import': 'Importing',\n",
       " 'export': 'Exporting',\n",
       " 'sermon': 'Judgment_direct_address',\n",
       " 'pronostiquer': 'Predicting',\n",
       " 'ambitionner': 'FR_Purpose',\n",
       " 'il_inspirer': 'FR_Influence_of_event_on_cognizer-Subjective_influence',\n",
       " 'dispute': 'Other_sense',\n",
       " 'acculer': 'FR_Reason',\n",
       " 'gémir': 'Complaining',\n",
       " 'réveiller': 'Other_sense',\n",
       " 'disputer': 'Other_sense',\n",
       " 'si_bien_que': 'Causation',\n",
       " 'observation': 'Other_sense',\n",
       " 'miser': 'FR_Reliance_on_expectation',\n",
       " 'en_bon_ordre': 'Other_sense',\n",
       " 'privatiser': 'Commerce_sell',\n",
       " 'répétition': 'Other_sense',\n",
       " 'astreindre': 'FR_Reason',\n",
       " 'unanime': 'Be_in_agreement_on_action',\n",
       " 'suggérer': 'FR_Attempt_suasion.to_do',\n",
       " 'amodiation': 'Renting',\n",
       " 'dans_un_but_de': 'FR_Means_for_purpose',\n",
       " 'repérer': 'FR_Becoming_aware',\n",
       " \"se_mettre_d'accord\": 'FR_Make_agreement_on_assessment',\n",
       " 'par_suite': 'Causation',\n",
       " 'proclamer': 'FR_Statement-manner-noise',\n",
       " 'se_décider': 'FR_Deciding',\n",
       " 'objecter': 'Communication_response',\n",
       " 'convier': 'FR_Attempt_suasion.to_do',\n",
       " 'vision': 'FR_Awareness-Certainty-Opinion',\n",
       " 'liquider': 'Commerce_sell',\n",
       " 'par_la_faute_de': 'Causation',\n",
       " 'bonification': 'Commerce_buy',\n",
       " 'approbation': 'Ratification',\n",
       " 'retombée': 'Causation',\n",
       " 'désignation': 'Appointing',\n",
       " 'avertissement': 'FR_Telling',\n",
       " 'stimulation': 'FR_Contingency-Objective_influence',\n",
       " 'étant_donné': 'Evidence',\n",
       " 'appelé': 'Referring_by_name',\n",
       " 'mettre_en_doute': 'FR_Expressing_truth_of_proposition',\n",
       " 'reversement': 'FR_Giving_money',\n",
       " 'du': 'Causation',\n",
       " 'prévisible': 'FR_Expectation',\n",
       " 'pronostic': 'Predicting',\n",
       " 'convergence': 'Be_in_agreement_on_assessment',\n",
       " 'haranguer': 'FR_Speak_on_topic',\n",
       " 'dit': 'FR_Being_named',\n",
       " 'riposte': 'Response',\n",
       " 'contre-attaque': 'Response',\n",
       " 'répliquer': 'Communication_response',\n",
       " 'téléphoner': 'FR_Contacting',\n",
       " \"s'arranger\": 'Make_agreement_on_action',\n",
       " 'faire_en_sorte': 'Causation',\n",
       " 'en_appeler': 'FR_Attempt_suasion.to_do',\n",
       " 'credo': 'FR_Awareness-Certainty-Opinion',\n",
       " 'inspiration': 'Other_sense',\n",
       " 'péage': 'Commerce_pay',\n",
       " 'atténuation': 'Other_sense',\n",
       " 'contestation': 'FR_Taking_sides',\n",
       " 'séquelle': 'Causation',\n",
       " 'se_plaindre': 'Complaining',\n",
       " 'affréter': 'Renting',\n",
       " 'sommer': 'FR_Request',\n",
       " 'se_vanter': 'Bragging',\n",
       " 'interprétation': 'FR_Awareness-Certainty-Opinion',\n",
       " 'décidé': 'FR_Deciding',\n",
       " 'exhorter': 'FR_Attempt_suasion.to_do',\n",
       " \"protocole_d'accord\": 'Be_in_agreement_on_action',\n",
       " 'lentille_de_contact': 'Other_sense',\n",
       " 'crier': 'FR_Statement-manner-noise',\n",
       " 'dans_quel_but': 'FR_Means_for_purpose',\n",
       " \"rappeler_à_l'ordre\": 'Judgment_direct_address',\n",
       " 'constatation': 'FR_Statement-manner-noise',\n",
       " 'présager': 'FR_Expectation',\n",
       " \"de_l'avis_de\": 'FR_Awareness-Certainty-Opinion',\n",
       " \"à_l'appel_de\": 'FR_Attempt_suasion.to_do',\n",
       " 'désaveu': 'FR_Expressing_truth_of_proposition',\n",
       " 'certifier': 'FR_Commitment.veracity',\n",
       " 'par_contrecoup': 'Causation',\n",
       " 'invitation': 'Other_sense',\n",
       " 'formuler': 'FR_Encoding',\n",
       " 'se_concerter': 'FR_Make_agreement_on_assessment',\n",
       " 'imprévu': 'FR_Expectation',\n",
       " 'pensée': 'Other_sense',\n",
       " 'à_des_fins': 'FR_Means_for_purpose',\n",
       " 'déceler': 'FR_Becoming_aware',\n",
       " 'commercer': 'FR_Commerce_scenario',\n",
       " 'aveu': 'Reveal_secret',\n",
       " 'à_force_de': 'Causation',\n",
       " 'encouragement': 'FR_Influence_of_event_on_cognizer-Subjective_influence',\n",
       " 'intuition': 'FR_Awareness-Certainty-Opinion',\n",
       " 'déduction': 'Other_sense',\n",
       " \"dans_l'optique_de\": 'FR_Means_for_purpose',\n",
       " 'discrédit': 'FR_Judgment_communication',\n",
       " 'étayer': 'Other_sense',\n",
       " 'convaincant': 'FR_Cognizer_affecting.veracity',\n",
       " 'préjugé': 'FR_Awareness-Certainty-Opinion',\n",
       " 'argumentation': 'FR_Attempt_supported_suasion',\n",
       " 'préjuger': 'FR_Expectation',\n",
       " 'validation': 'Other_sense',\n",
       " 'se_perdre': 'Other_sense',\n",
       " 'tractation': 'FR_Chatting-Discussion',\n",
       " 'être_fonction_de': 'FR_Contingency-Objective_influence',\n",
       " 'initiateur': 'FR_Cause_to_start-Launch_process',\n",
       " \"force_de_l'ordre\": 'Other_sense',\n",
       " 'délibérer': 'Other_sense',\n",
       " 'contrecoup': 'Causation',\n",
       " 'à_juste_raison': 'Other_sense',\n",
       " 'évocation': 'FR_Speak_on_topic',\n",
       " 'se_lamenter': 'Complaining',\n",
       " 'discourir': 'FR_Speak_on_topic',\n",
       " 'dissuader': 'FR_Cognizer_affecting.to_do',\n",
       " 'ordre_public': 'Other_sense',\n",
       " 'gager': 'FR_Expectation',\n",
       " 'récrimination': 'Complaining',\n",
       " 'nationalisation': 'Commerce_buy',\n",
       " 'avoir_à_revendre': 'Other_sense',\n",
       " 'remémorer': 'FR_Telling',\n",
       " 'infléchir': 'FR_Contingency-Objective_influence',\n",
       " 'scepticisme': 'FR_Awareness-Certainty-Opinion',\n",
       " 'professer': 'FR_Statement-manner-noise',\n",
       " 'rétribution': 'Commerce_pay',\n",
       " 'balbutier': 'Other_sense',\n",
       " \"voir_d'un_oeil_bienveillant\": 'FR_Being_in_favor_of',\n",
       " 'sensibilisation': 'FR_Attempt_suasion.legitimacy',\n",
       " 'influer': 'FR_Contingency-Objective_influence',\n",
       " 'ferrailler': 'Quarreling',\n",
       " 'dédain': 'Judgment',\n",
       " 'induction': 'Other_sense',\n",
       " 'à_tel_point_que': 'Causation',\n",
       " 'prier': 'Other_sense',\n",
       " 'sensibiliser': 'FR_Attempt_suasion.legitimacy',\n",
       " 'riposter': 'Response',\n",
       " 'prêcher': 'FR_Attempt_suasion.legitimacy',\n",
       " 'hurler': 'FR_Statement-manner-noise',\n",
       " 'recouvrer': 'FR_Getting_money',\n",
       " 'plaidoirie': 'FR_Attempt_supported_suasion',\n",
       " 'déprécier': 'FR_Judgment_communication',\n",
       " 'avoir_raison': 'Other_sense',\n",
       " 'dénier': 'FR_Grant_permission-Permitting',\n",
       " 'par_voie_de_conséquence': 'Causation',\n",
       " 'réprimander': 'Judgment_direct_address',\n",
       " 'se_disputer': 'Quarreling',\n",
       " 'deviner': 'Coming_to_believe',\n",
       " 'à_point_nommé': 'Other_sense',\n",
       " 'injonction': 'FR_Request',\n",
       " 'vouloir_pour_preuve': 'FR_Attempt_supported_suasion',\n",
       " 'légitimation': 'FR_Justifying',\n",
       " 'en_vertu_duquel': 'Other_sense',\n",
       " 'compliment': 'Judgment_direct_address',\n",
       " 'ronchonner': 'Complaining',\n",
       " 'dissuasion': 'FR_Cognizer_affecting.to_do',\n",
       " 'donner_à_penser': 'Evidence',\n",
       " 'délibération': 'FR_Chatting-Discussion',\n",
       " 'dialogue_de_sourd': 'FR_Chatting-Discussion',\n",
       " 'aviser': 'Other_sense',\n",
       " 'réflexe': 'Other_sense',\n",
       " 'ressort': 'Other_sense',\n",
       " 'ralliement': 'FR_Being_in_favor_of',\n",
       " 'cataloguer': 'Categorization',\n",
       " \"avec_l'objectif_de\": 'FR_Means_for_purpose',\n",
       " 'dénigrement': 'FR_Judgment_communication',\n",
       " 'marmonner': 'FR_Statement-manner-noise',\n",
       " 'donner_à_croire': 'FR_Attempt_suasion.veracity',\n",
       " 'réclamation': 'Other_sense',\n",
       " 'proclamation': 'FR_Statement-manner-noise',\n",
       " 'résolu': 'FR_Purpose',\n",
       " 'présumer': 'FR_Awareness-Certainty-Opinion',\n",
       " \"sur_l'injonction_de\": 'Response',\n",
       " \"raison_d'état\": 'Other_sense',\n",
       " 'se_confier': 'Reveal_secret',\n",
       " 'maudire': 'FR_Judgment_communication',\n",
       " 'chanter': 'FR_Statement-manner-noise',\n",
       " 'exaltation': 'Other_sense',\n",
       " 'déchaîner': 'Other_sense',\n",
       " 'renonciation': 'FR_Renunciation',\n",
       " 'facturer': 'Commerce_collect',\n",
       " 'brouille': 'Quarreling',\n",
       " 'sommation': 'FR_Request',\n",
       " 'causerie': 'FR_Chatting-Discussion',\n",
       " 'influençable': 'Other_sense',\n",
       " 'chanter_les_louanges': 'FR_Judgment_communication',\n",
       " 'désapprobation': 'Judgment',\n",
       " 'apostropher': 'FR_Hail',\n",
       " 'confesser': 'Other_sense',\n",
       " \"s'aviser\": 'Other_sense',\n",
       " 'joindre': 'Other_sense',\n",
       " 'reparler': 'FR_Speak_on_topic',\n",
       " 'disposé': 'Willingness',\n",
       " 'persuasion': 'FR_Cognizer_affecting.veracity',\n",
       " 'récuser': 'FR_Taking_sides',\n",
       " 'sonder': 'Questioning',\n",
       " 'faire_valoir': 'Other_sense',\n",
       " 'recouvrement': 'FR_Getting_money',\n",
       " \"c'est_que\": 'Explaining_the_facts',\n",
       " 'proférer': 'FR_Making_speech',\n",
       " 'promulgation': 'Ratification',\n",
       " 'épingler': 'FR_Judgment_communication',\n",
       " 'corroborer': 'Evidence',\n",
       " 'plus_que_de_raison': 'Other_sense',\n",
       " 'divulgation': 'Reveal_secret',\n",
       " 'en_ordre': 'Other_sense',\n",
       " 'classifier': 'Categorization',\n",
       " 'estime': 'Judgment',\n",
       " 'du_fait_que': 'Causation',\n",
       " 'faire_naître': 'Other_sense',\n",
       " 'implorer': 'FR_Request',\n",
       " 'grogner': 'FR_Statement-manner-noise'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_most_frequent_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPbOlFK9nLoU"
   },
   "source": [
    "## Modèle et tokenization de type *BERT\n",
    "\n",
    "On va utiliser un modèle pré-entraîné de type *BERT, en passant par le module \"transformers\" d'huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4DfehySexyR",
    "outputId": "75f28db4-c81c-4a21-cc61-26454db8915a"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import transformers\n",
    "except ImportError:\n",
    "  !pip install transformers\n",
    "\n",
    "try:\n",
    "  import sacremoses\n",
    "except ImportError:\n",
    "  !pip install sacremoses\n",
    "\n",
    "  \n",
    "# les modules permettant de charger un modèle (resp. un tokenizer / une config)\n",
    "# et de repérer le type d'instance automatiquement d'après le nom du modèle\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166,
     "referenced_widgets": [
      "a5611e1877c246e88f8c9ba7f73fcd14",
      "2a63baf0cdab419d985d39d07e0b376c",
      "b43699041dea4a7bb0f97ccf078e5c8b",
      "bc909bc1b05b4067a947a03dd2708a17",
      "6bebeb3da1ff494196ff8b3c5beb3cff",
      "a15458eba4fe4a3b93dcc3f9494990e3",
      "fd9fe2c7ea1c4d3b98de684243201383",
      "21b7360121934bc392a719c6eedef583",
      "cb2114aab6ab480cb6023fa0e5578c28",
      "ea2cbc25c2504383bbe9ecd164448d84",
      "67034dd7a099431d92976a1c2d237e31",
      "ff41d7c375274c64931a86c6d8c7a0bb",
      "70315c6a722e4176a2af4b631c755d36",
      "adb58c36931a412e802c295694410b9c",
      "1ffee7d44a3347adbc88c47208679bd6",
      "8fd286d4947644c5b7f1cc9cb8f5b179",
      "719b612ae5ab417d81876ae8744cf85f",
      "39f9c710ebd84b6fa81c4b9205c18a15",
      "23f0f80a2faa4228a1897b5b92562fff",
      "200efeb1a36f47b888af30721f1e8a97",
      "062aa7e54cce40c99ca2e1043f134b90",
      "ced61c53666f45f48fe301f2af4e5e35",
      "496703edea874c6fb8a22fd32e456081",
      "0a05f98a9e1a48729e833652dc413be5"
     ]
    },
    "id": "JJiKhfxhexyV",
    "outputId": "105f47fb-8666-4bb8-f59b-5e1538d231ae"
   },
   "outputs": [],
   "source": [
    "# On choisit de travailler avec le modèle FlauBERT\n",
    "# cf. liste des modèles dispos : https://huggingface.co/transformers/pretrained_models.html\n",
    "\n",
    "# on charge ici le tokenizer Flaubert\n",
    "# et la config du modèle\n",
    "flaubert_tokenizer = AutoTokenizer.from_pretrained(\"flaubert/flaubert_base_cased\")\n",
    "flaubert_config = AutoConfig.from_pretrained(\"flaubert/flaubert_base_cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMmtutNBexyQ"
   },
   "source": [
    "### Encodage des données (correspondance entre rang de mot et rang de token \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Pour pouvoir utiliser un modèle *BERT pré-entraîné, il faut\n",
    "- utiliser la même tokenisation en tokens (potentiellement des sous-mots) que celle utilisée à l'entraînement du modèle\n",
    "- convertir les séquences de tokens en séquences d'ids de tokens \n",
    "- et maintenir un lien entre les rangs de mot dans la phrase (dont le rang du target) et les rangs de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GuLEbwS_tVtZ",
    "outputId": "5c89f1fb-2f48-4674-f6c6-e7be71cee1c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not add padding and not add special tokens : \n",
      "Len = 100 target token rank = 6 tid_seq = [0, 1198, 17358, 13299, 121, 5677, 18719, 16724, 23, 3842, 16, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Len = 100 target token rank = 4 tid_seq = [0, 158, 5213, 15, 965, 22, 14659, 896, 16, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Len = 100 target token rank = 6 tid_seq = [0, 59, 51, 34, 42, 83, 681, 20, 1138, 82, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "class WSDEncoder:\n",
    "    def __init__(self, tokenizer, config):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config # pour récupérer les indices des tokens spéciaux\n",
    "    \n",
    "    def new_ranks(self, sentences, tg_works):\n",
    "\n",
    "      tg_trks = []\n",
    "      phrases = []\n",
    "      \n",
    "\n",
    "      for sent, rank in zip(sentences, tg_works):\n",
    "        has_seen = False\n",
    "       \n",
    "        word_gold = sent[rank]\n",
    "        phrase = []\n",
    "        for index, word in enumerate(sent):\n",
    "          if word == word_gold and has_seen == False:\n",
    "            tg_trks.append(len(phrase)+1)#+1 Because of the eventual 'beginning of sentence' token\n",
    "            has_seen = True\n",
    "          phrase.extend(flaubert_tokenizer.tokenize(word))\n",
    "        phrases.append(phrase)\n",
    "      \n",
    "      return phrases, tg_trks\n",
    " \n",
    "    \n",
    "    def encode(self, sentences, tg_wrks, max_length=350, verbose=False, is_split_into_words=True):\n",
    "      \"\"\" \n",
    "      Input: \n",
    "        - sentences : list of sentences\n",
    "           -- if is_split_into_words:\n",
    "              sentences are already split into words \n",
    "              (hence sentences = list of word strings [[w1, w2, w3], [w1, w2]...])\n",
    "           -- otherwise, sentences are to split on spaces to get words\n",
    "\n",
    "        - tg_wrks : list of the ranks of target words\n",
    "          (one rank per sentence, starting at 0 in a sentence)\n",
    "        - max_length : maximum length in number of tokens\n",
    "\n",
    "      Returns:\n",
    "        - tid_seqs : the sentences padded/truncated so that each contains max_length token ids\n",
    "        - first_trk_of_targets : for each sentence, \n",
    "                                 the rank in corresponding tid_seq\n",
    "                                 of the first token of the target word\n",
    "      Example\n",
    "      sentences = ['Conséquemment , nous comprendrions .']\n",
    "      tg_wrks = [3]\n",
    "\n",
    "      if the sentence is tokenized into \n",
    "        '<s>', 'Con', 'séqu', 'emment</w>', ',</w>', 'nous</w>', 'compr', 'end', 'rions</w>', '.</w>' ....\n",
    "      the first token rank of the target \"comprendrions\" is 6 ('compr')\n",
    "\n",
    "      \"\"\"\n",
    "      if is_split_into_words == False:\n",
    "        sentences = [sentence.split() for sentence in sentences] #Splitting sentences on spaces\n",
    "      \n",
    "     \n",
    "\n",
    "      phrases, first_trk_of_targets = self.new_ranks(sentences, tg_wrks)\n",
    "      \n",
    "      \n",
    "      tokenized = []\n",
    "   \n",
    "      for phrase in phrases:\n",
    "         tokenized.append(flaubert_tokenizer.encode(phrase,add_special_tokens = True,truncation = True, max_length = max_length, padding = 'max_length', pad_to_max_length = True))\n",
    "     \n",
    "      #Encoding lemmas\n",
    "\n",
    "      \n",
    "      # TODO HERE : encoding method\n",
    "\n",
    "      # Indications:\n",
    "      # 1. apply flaubert tokenization word per word, and build\n",
    "      #    tid_seqs first without padding / truncation nor special tokens,\n",
    "      #    and keep track of token rank of first token of target word\n",
    "      # 2. then truncate and pad, and add special symbols\n",
    "      # (write several methods for easier reading)\n",
    "      \n",
    "      return tokenized, first_trk_of_targets\n",
    "\n",
    "encoder = WSDEncoder(flaubert_tokenizer, flaubert_config)\n",
    "\n",
    "# test encoder\n",
    "test_sents = [\"Conséquemment leurs codes comprendraient des erreurs .\",\n",
    "            \"J' essaie de comprendre les transformers .\",  \n",
    "            \"Il n' a pas bien compris le code !\"]\n",
    "# les mots target sont les occurrences de du verbe \"comprendre\"\n",
    "test_tg_wrks = [3, 3, 5] # En réalité pour la première phrase comprendraient se situe au troisième\n",
    "\n",
    "# TODO: décommenter pour tester votre méthode encode\n",
    "\n",
    "# 1. Not add padding and not add special tokens\n",
    "\n",
    "print(\"Not add padding and not add special tokens : \")\n",
    "tid_seqs, first_trk_of_targets = encoder.encode(test_sents, test_tg_wrks, max_length=100,is_split_into_words=False)\n",
    "#print(len(tid_seqs),\" \",len(first_trk_of_targets))\n",
    "for tid_seq, ft in zip(tid_seqs, first_trk_of_targets):\n",
    "    print(\"Len = %d target token rank = %d tid_seq = %s\" % (len(tid_seq), ft, str(tid_seq))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ySTnpTLexyi"
   },
   "source": [
    "#### Test encodage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qyWWzPl91PB",
    "outputId": "c2e6a87c-c419-43b1-b558-f72b6192af58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trgs [6]\n",
      "['<s>', 'Con', 'séqu', 'emment</w>', 'leurs</w>', 'codes</w>', 'compr', 'endraient</w>', 'des</w>', 'erreurs</w>', '.</w>', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Len = 20 target token rank = 6 tid_seq = [0, 1198, 17358, 13299, 121, 5677, 18719, 16724, 23, 3842, 16, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "encoder = WSDEncoder(flaubert_tokenizer, flaubert_config)\n",
    "\n",
    "# test encoder\n",
    "\n",
    "#@@ erreur dans le test\n",
    "#test_sents = [\"Conséquemment\", \"leurs\", \"codes\", \"comprendraient\", \"des\", \"erreurs\", \".\"]\n",
    "test_sents = [[\"Conséquemment\", \"leurs\", \"codes\", \"comprendraient\", \"des\", \"erreurs\", \".\"]]\n",
    "                    \n",
    "# les mots target sont les occurrences de du verbe \"comprendre\"\n",
    "#test_tg_wrks = [2]\n",
    "test_tg_wrks = [3]\n",
    "\n",
    "# TODO: décommenter pour tester votre méthode encode\n",
    "tid_seqs, first_trk_of_targets = encoder.encode(test_sents, test_tg_wrks, max_length=20, verbose=True, is_split_into_words = True)\n",
    "\n",
    "print('trgs',first_trk_of_targets)\n",
    "\n",
    "for tid_seq, ft in zip(tid_seqs, first_trk_of_targets):\n",
    "  #@@ plus de traces\n",
    "  print(flaubert_tokenizer.convert_ids_to_tokens(tid_seq))\n",
    "  print(\"Len = %d target token rank = %d tid_seq = %s\" % (len(tid_seq), ft, str(tid_seq)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "n2ER6-SXhEj2",
    "outputId": "6f5f3347-e129-41b6-afde-8fba507adbd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autorisé</w>'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.tokenizer.convert_ids_to_tokens(6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHApJ8mgexyn"
   },
   "source": [
    "### Classe WSDData: encodage complet des données asfalda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "qMxpzaBhexyo"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class WSDData:\n",
    "    def __init__(self, corpus_type, sentences, tg_wrks, tg_lemmas, labels, encoder, max_length=350):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - corpus type string (train/dev/test/val)\n",
    "        - list of sentences (each sentence = list of word strings)\n",
    "        - list of target word ranks : one per sentence\n",
    "        - list of gold label id\n",
    "        - encoder = instance of WSDEncoder\n",
    "\n",
    "        - max_length = size of encoded sequences, in nb of bert tokens \n",
    "                      (padded / truncated via encoder.encode)\n",
    "    \n",
    "        Encodes all the data using the relevant identifiers\n",
    "        \"\"\"\n",
    "        \n",
    "        self.corpus_type = corpus_type # train / dev / test / val\n",
    "        self.size = len(sentences)\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.labels = labels       # gold label ids\n",
    "        self.sentences = sentences # list of list of word strings\n",
    "        self.tg_lemmas = tg_lemmas #list of target lemmas\n",
    "        \n",
    "        tid_seqs, first_trk_of_targets = encoder.encode(sentences, tg_wrks, max_length)\n",
    "        self.tg_lemma_indexes = [lemma2i[lemma]for lemma in self.tg_lemmas]\n",
    "\n",
    "        self.tid_seqs = tid_seqs  # sequences of token ids\n",
    "        self.first_trk_of_targets  = first_trk_of_targets   # target token ranks\n",
    "        \n",
    "        \n",
    "\n",
    "    def shuffle(self):\n",
    "      \"\"\"\n",
    "      Rearranges all the data in a new random order\n",
    "      (sentences, tg_lemmas, tg_trks, tid_seqs, labels)\n",
    "\n",
    "      NB: ** original order is lost **\n",
    "      \"\"\"\n",
    "      z = list(zip(self.labels, self.sentences, self.tg_lemma_indexes, self.tid_seqs, self.first_trk_of_targets))\n",
    "      random.shuffle(z)\n",
    "      labels, sentences, tg_lemma_indexes, tid_seqs,first_trk_of_targets = zip(*z)\n",
    "      \n",
    "\n",
    "      \n",
    "      return labels, sentences, tg_lemma_indexes, tid_seqs,first_trk_of_targets\n",
    "\n",
    " \n",
    "\n",
    "    # production de batches de données\n",
    "    def make_batches(self, batch_size, shuffle_data=False):\n",
    "        \"\"\"\n",
    "        Returns an iterator over 3 torch tensors \n",
    "        - batch of token id sequences\n",
    "        - corresponding batch of target token ranks\n",
    "        - corresponding batch of labels for these targets\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        # (use \"yield\" function to return iterator\n",
    "        bstart = 0\n",
    "        if shuffle_data:\n",
    "          self.labels, self.sentences, self.tg_lemmas, self.tid_seqs, self.first_trk_of_targets = self.shuffle()\n",
    "        N = len(self.labels)\n",
    "        while bstart < len(self.labels):\n",
    "          bend = min(bstart+batch_size,N)\n",
    "          b_labels, b_tid_seqs, b_tg_trks, b_lemmas = self.labels[bstart:bend] , self.tid_seqs[bstart:bend] , self.first_trk_of_targets[bstart:bend], self.tg_lemma_indexes[bstart:bend] \n",
    "          assert(len(b_labels)==len(b_tid_seqs))\n",
    "          yield (b_tid_seqs, b_tg_trks, b_labels, b_lemmas)#lemmas\n",
    "        \n",
    "          bstart += batch_size\n",
    "\n",
    "       \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z6sSDJMarsXA",
    "outputId": "b94f11ec-520d-4d12-9419-2b7831f34610"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dev', 'train', 'test', 'val'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6D81AP4ye10",
    "outputId": "fcb341d3-576c-4c61-9a22-25914f804985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodage de la partie dev ...\n",
      "Encodage de la partie train ...\n",
      "Encodage de la partie test ...\n",
      "Encodage de la partie val ...\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 300\n",
    "wsd_data = {}\n",
    "# key = part of the split corpus (train/test/dev/val)\n",
    "for p in sentences.keys():\n",
    "    print(\"Encodage de la partie %s ...\" % p)\n",
    "    wsd_data[p] = WSDData(p, sentences[p], tg_wrks[p], tg_lemmas[p], labels[p], \n",
    "                          encoder, max_length=MAX_LENGTH)\n",
    "    # vérif que l'encodage donne bien la bonne taille\n",
    "    for i, s in enumerate(wsd_data[p].tid_seqs):\n",
    "        if len(s) != MAX_LENGTH:\n",
    "            print(\"Size bug:\", i, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIw1BE1wexys"
   },
   "source": [
    "## Classe WSDClassifier : le réseau de neurones pour la WSD\n",
    "\n",
    "Architecture de base = \n",
    "- le modèle *BERT (ici FlauBERT)\n",
    "- puis une couche linéaire + softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQ-w08okgaXv"
   },
   "source": [
    "### Le réseau : architecture, propagation avant, évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "KYAvfY83z0dF"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  def __init__(self,input_size,output_size,hidden_size):\n",
    "    super(MLP, self).__init__()\n",
    "    self.encoder = nn.Linear(input_size,hidden_size)\n",
    "    self.decoder = nn.Linear(hidden_size,output_size)\n",
    "    self.activation = nn.Tanh()\n",
    "  def forward(self,xinput):\n",
    "    h = self.activation(self.encoder(xinput))\n",
    "    return self.decoder(h)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "gFQEpfkRexyy"
   },
   "outputs": [],
   "source": [
    "\n",
    "class WSDClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_labels, device='cpu', bert_model_name=\"flaubert/flaubert_base_cased\", freeze_bert = True, use_mlp = False, hidden_size = 100,nbr_lemmas = len(lemma2i), lemma_embedding_size = 518, add_lemmas = False):\n",
    "        super(WSDClassifier, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.use_mlp = use_mlp\n",
    "        self.add_lemmas = add_lemmas\n",
    "                \n",
    "        # le début du réseau est un réseau de type *BERT\n",
    "        # le .to(device) déclenche la copie vers un éventuel GPU\n",
    "        self.bert_layer = AutoModel.from_pretrained(bert_model_name,\n",
    "                                                   ).to(self.device)\n",
    "        # on récupère la config pour avoir la taille des embeddings bert\n",
    "        self.bert_config = AutoConfig.from_pretrained(bert_model_name)\n",
    "        \n",
    "        if self.add_lemmas:\n",
    "          #Adding lemma information\n",
    "          self.hidden_size_bert = int(self.bert_config.hidden_size)+int(lemma_embedding_size)\n",
    "          print('lemmahidden', self.hidden_size_bert)\n",
    "        else:\n",
    "          self.hidden_size_bert = int(self.bert_config.hidden_size)\n",
    "        if add_lemmas:\n",
    "          self.lemma_embedding = nn.Embedding(nbr_lemmas, lemma_embedding_size).to(self.device)\n",
    "        \n",
    "        #print('hidden',hidden_size)\n",
    "        # TODO HERE : la suite\n",
    "        # TODO: implement option freeze_bert\n",
    "        if freeze_bert:\n",
    "          for param in self.bert_layer.parameters(): #Freezing the Bert parameters\n",
    "            param.requires_grad = False\n",
    "        if self.use_mlp:\n",
    "          self.mlp = MLP(self.hidden_size_bert,num_labels,100).to(self.device)\n",
    "\n",
    "        else:\n",
    "          self.linear = torch.nn.Linear(self.hidden_size_bert,num_labels).to(self.device)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim = 1).to(self.device)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, b_tid_seq, b_tg_trk, b_tg_lemma_indexes = None):\n",
    "        \"\"\"\n",
    "        Inputs: (all are tensors, on the relevant device)\n",
    "            - a batch of sentences = a batch of token id sequences \n",
    "              (as output in 'input_ids' member of tokenizer output)\n",
    "            - a batch of target token rank = for each of the sentences, \n",
    "              the rank of first token of the target word to disambiguate\n",
    "\n",
    "        Output: log_softmax scores for the whole batch (batch_size x num_labels)\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.main_part = torch.nn.Sequential(\n",
    "            self.bert_layer,\n",
    "            self.linear,\n",
    "            self.softmax)\n",
    "        \"\"\"\n",
    "        \n",
    "     \n",
    "    \n",
    "        embeddings_bert = self.bert_layer(b_tid_seq, return_dict = True).last_hidden_state.to(self.device)\n",
    "        \n",
    "       \n",
    "      \n",
    "     \n",
    "      \n",
    "        \n",
    "        embeddings_bert_tgt = embeddings_bert[torch.arange(embeddings_bert.size(0)), b_tg_trk].to(self.device) #last hidden state, ranks. We extract the \n",
    "        #necessary contextualised embeddings from the last layer\n",
    "        \n",
    "        if self.add_lemmas: #add lemma information\n",
    "          lemma_embeddings = self.lemma_embedding(b_tg_lemma_indexes).to(self.device)\n",
    "          embeddings_bert_tgt = torch.cat((embeddings_bert_tgt,lemma_embeddings), dim = 1).to(self.device)#If we use lemmas, we concat bert embeddings and information about lemmas\n",
    "        \n",
    "\n",
    "        \n",
    "        if self.use_mlp:\n",
    "          linear_tgt = self.mlp(embeddings_bert_tgt).to(self.device)\n",
    "        else:\n",
    "          linear_tgt = self.linear(embeddings_bert_tgt).to(self.device)\n",
    "        \n",
    "        soft_tgt = self.softmax(linear_tgt).to(self.device)\n",
    "       \n",
    "        \n",
    "        return soft_tgt\n",
    "        #idx = torch.tensor([1, 2])\n",
    "        #x[torch.arange(x.size(0)), idx]\n",
    "     \n",
    "        \n",
    "        # TODO HERE\n",
    "        #  - récuperer les embeddings *bert de tous les tokens des phrases du batch \n",
    "        #    [ batch_size * seq_len * bert_emb_size ]\n",
    "        #\n",
    "        #  - isoler l'embedding du token target pour toutes les phrases du batch\n",
    "        #    [ batch_size * bert_emb_size ]\n",
    "\n",
    "        #\n",
    "        #    Indications pour le faire élégamment et efficacement:\n",
    "        #    https://discuss.pytorch.org/t/how-to-select-specific-vector-in-3d-tensor-beautifully/37724\n",
    "        #\n",
    "        #  - et suite pour fine tuning\n",
    "            \n",
    "    def run_on_dataset(self, wsd_data, optimizer, batch_size=32, validation_use = False):\n",
    "        \"\"\"\n",
    "        Run classifier on wsd_data and compute accuracy\n",
    "        Inputs = \n",
    "         - wsd_data (WSDDataset instance)\n",
    "         - batch_size\n",
    "        Returns:\n",
    "         - list of predicted label ids\n",
    "        \"\"\"\n",
    "        pred_labels = []\n",
    "        val_losses = []\n",
    "        batch_acc = []\n",
    "        \n",
    "        loss_function = nn.NLLLoss()\n",
    "\n",
    "\n",
    "        \n",
    "        # toggle evaluation mode of the model (IMPORTANT)\n",
    "        self.eval()\n",
    "        for b_tid_seqs, b_tg_trks, b_labels, b_lemma_idx in wsd_data.make_batches(32, shuffle_data=False):\n",
    "          with torch.no_grad():\n",
    "            b_tid_seqs = torch.tensor(b_tid_seqs, device=self.device).to(self.device)\n",
    "            \n",
    "            b_tg_trks = torch.tensor(b_tg_trks, device=self.device).to(self.device)\n",
    "            b_labels = torch.tensor(b_labels, device=self.device).to(self.device)\n",
    "            b_lemma_idx = torch.tensor(b_lemma_idx, device=self.device).to(self.device)\n",
    "            log_probs = classifier(b_tid_seqs, b_tg_trks, b_lemma_idx).to(self.device)\n",
    "        \n",
    "            log_probs = self(b_tid_seqs, b_tg_trks,b_lemma_idx)\n",
    "            b_pred_labels = torch.argmax(log_probs, dim=1).to(self.device)\n",
    "            \n",
    "            pred_labels.extend(b_pred_labels)\n",
    "            \n",
    "            if validation_use:\n",
    "            \n",
    "              loss = loss_function(log_probs,b_labels)\n",
    "    \n",
    "              val_losses.append(loss.item())\n",
    "          \n",
    "          batch_acc.append(self.evaluate(b_labels,b_pred_labels))\n",
    "          \n",
    "\n",
    "\n",
    "        \n",
    "     \n",
    "        return pred_labels, val_losses, mean(batch_acc), b_labels, batch_acc\n",
    "\n",
    "    def evaluate(self, gold_labels, pred_labels):\n",
    "        \"\"\" returns accuracy, nb_correct, nb_total \"\"\"\n",
    "        \n",
    "        acc = float(torch.sum(gold_labels == pred_labels))/len(gold_labels)\n",
    "        return acc\n",
    "\n",
    "        # TODO \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrkbQ4hVexy1",
    "outputId": "34d2ff8c-40b8-42e6-e28a-ffc867c22a0a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at flaubert/flaubert_base_cased were not used when initializing FlaubertModel: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "- This IS expected if you are initializing FlaubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAM named bert_layer.position_embeddings.weight, of shape torch.Size([512, 768])\n",
      "False\n",
      "PARAM named bert_layer.embeddings.weight, of shape torch.Size([68729, 768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm_emb.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm_emb.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.0.q_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.0.q_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.0.k_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.0.k_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.0.v_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.0.v_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.0.out_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.0.out_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.1.q_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.1.q_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.1.k_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.1.k_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.1.v_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.1.v_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.1.out_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.1.out_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.2.q_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.2.q_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.2.k_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.2.k_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.2.v_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.2.v_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.2.out_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.2.out_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.3.q_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.3.q_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.3.k_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.3.k_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.3.v_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.3.v_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.3.out_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.3.out_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.4.q_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.4.q_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.4.k_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.4.k_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.4.v_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.4.v_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.4.out_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.4.out_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.5.q_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.5.q_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.5.k_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.5.k_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.5.v_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.5.v_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.5.out_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.5.out_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.6.q_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.6.q_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.6.k_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.6.k_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.6.v_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.6.v_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.6.out_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.6.out_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.7.q_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.7.q_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.7.k_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.7.k_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.7.v_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.7.v_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.7.out_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.7.out_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.8.q_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.8.q_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.8.k_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.8.k_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.8.v_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.8.v_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.8.out_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.8.out_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.9.q_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.9.q_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.9.k_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.9.k_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.9.v_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.9.v_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.9.out_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.9.out_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.10.q_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.10.q_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.10.k_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.10.k_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.10.v_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.10.v_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.10.out_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.10.out_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.11.q_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.11.q_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.11.k_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.11.k_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.11.v_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.11.v_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.11.out_lin.weight, of shape torch.Size([768, 768])\n",
      "False\n",
      "PARAM named bert_layer.attentions.11.out_lin.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.0.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.0.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.1.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.1.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.2.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.2.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.3.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.3.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.4.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.4.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.5.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.5.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.6.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.6.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.7.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.7.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.8.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.8.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.9.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.9.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.10.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.10.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.11.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm1.11.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.0.lin1.weight, of shape torch.Size([3072, 768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.0.lin1.bias, of shape torch.Size([3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.0.lin2.weight, of shape torch.Size([768, 3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.0.lin2.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.1.lin1.weight, of shape torch.Size([3072, 768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.1.lin1.bias, of shape torch.Size([3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.1.lin2.weight, of shape torch.Size([768, 3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.1.lin2.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.2.lin1.weight, of shape torch.Size([3072, 768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.2.lin1.bias, of shape torch.Size([3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.2.lin2.weight, of shape torch.Size([768, 3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.2.lin2.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.3.lin1.weight, of shape torch.Size([3072, 768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.3.lin1.bias, of shape torch.Size([3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.3.lin2.weight, of shape torch.Size([768, 3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.3.lin2.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.4.lin1.weight, of shape torch.Size([3072, 768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.4.lin1.bias, of shape torch.Size([3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.4.lin2.weight, of shape torch.Size([768, 3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.4.lin2.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.5.lin1.weight, of shape torch.Size([3072, 768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.5.lin1.bias, of shape torch.Size([3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.5.lin2.weight, of shape torch.Size([768, 3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.5.lin2.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.6.lin1.weight, of shape torch.Size([3072, 768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.6.lin1.bias, of shape torch.Size([3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.6.lin2.weight, of shape torch.Size([768, 3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.6.lin2.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.7.lin1.weight, of shape torch.Size([3072, 768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.7.lin1.bias, of shape torch.Size([3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.7.lin2.weight, of shape torch.Size([768, 3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.7.lin2.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.8.lin1.weight, of shape torch.Size([3072, 768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.8.lin1.bias, of shape torch.Size([3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.8.lin2.weight, of shape torch.Size([768, 3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.8.lin2.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.9.lin1.weight, of shape torch.Size([3072, 768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.9.lin1.bias, of shape torch.Size([3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.9.lin2.weight, of shape torch.Size([768, 3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.9.lin2.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.10.lin1.weight, of shape torch.Size([3072, 768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.10.lin1.bias, of shape torch.Size([3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.10.lin2.weight, of shape torch.Size([768, 3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.10.lin2.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.11.lin1.weight, of shape torch.Size([3072, 768])\n",
      "False\n",
      "PARAM named bert_layer.ffns.11.lin1.bias, of shape torch.Size([3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.11.lin2.weight, of shape torch.Size([768, 3072])\n",
      "False\n",
      "PARAM named bert_layer.ffns.11.lin2.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.0.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.0.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.1.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.1.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.2.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.2.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.3.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.3.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.4.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.4.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.5.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.5.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.6.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.6.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.7.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.7.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.8.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.8.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.9.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.9.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.10.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.10.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.11.weight, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named bert_layer.layer_norm2.11.bias, of shape torch.Size([768])\n",
      "False\n",
      "PARAM named linear.weight, of shape torch.Size([106, 768])\n",
      "True\n",
      "PARAM named linear.bias, of shape torch.Size([106])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# une instance de WSDClassifier\n",
    "num_labels = len(i2label)\n",
    "classifier = WSDClassifier(num_labels, device = 'cuda')\n",
    "\n",
    "#en décommentant, on voit le nb impressionnant de paramètres du modèle *BERT ...\n",
    "for name, param in classifier.named_parameters():\n",
    "  print(\"PARAM named %s, of shape %s\" % (name, str(param.shape)))\n",
    "  print(param.requires_grad)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LeXSxEXexy5"
   },
   "source": [
    "#### Test de la propagation avant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P4iIZvzDexy7",
    "outputId": "720d7ada-24a6-4ed7-f6a7-6e9ddd19d6bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size :  torch.Size([32, 300])  reference size :  torch.Size([32])\n",
      "output size :  torch.Size([32, 106])  reference size :  torch.Size([32])\n",
      "GOLD LABEL of first ex 64 ( = Other_sense)\n",
      "LOG_PROBS before training: tensor([-6.4051, -3.7102, -6.4820, -5.3021, -5.9793, -5.8952, -4.6454, -4.2802,\n",
      "        -3.3121, -4.9739, -6.2555, -6.3175, -4.5475, -5.4567, -5.2216, -5.7345,\n",
      "        -6.2958, -4.1462, -5.1009, -7.6405, -4.5653, -5.0407, -5.6026, -4.1740,\n",
      "        -5.6059, -3.3927, -3.6226, -4.9730, -5.9341, -5.4804, -4.6605, -6.7458,\n",
      "        -3.9760, -5.0839, -6.4270, -5.6717, -6.1434, -4.8073, -4.1461, -3.7406,\n",
      "        -5.1708, -5.1331, -4.8694, -6.9416, -5.8696, -5.0755, -6.6913, -5.6032,\n",
      "        -5.1722, -5.6508, -4.2339, -5.0959, -4.2807, -4.1126, -7.2889, -6.1083,\n",
      "        -5.6182, -5.1170, -6.6707, -5.2679, -4.5221, -3.8051, -3.9274, -4.2412,\n",
      "        -5.1580, -5.6849, -3.7225, -5.7134, -6.1165, -4.5911, -5.8958, -5.4651,\n",
      "        -5.2120, -6.5785, -5.7620, -5.1853, -4.9260, -5.1899, -4.6447, -6.4641,\n",
      "        -4.8353, -3.5110, -4.6938, -4.8675, -6.0374, -4.8463, -3.6413, -4.1165,\n",
      "        -6.0750, -2.5522, -5.5970, -4.8997, -4.5558, -5.6091, -3.9620, -4.3353,\n",
      "        -4.5488, -4.6088, -4.6025, -4.9387, -5.6579, -2.7729, -5.3793, -6.5996,\n",
      "        -5.0516, -5.9940], device='cuda:0')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# inutile de calculer les gradients\n",
    "with torch.no_grad():\n",
    "    # mode evaluation et pas train\n",
    "    classifier.eval()\n",
    "    for b_tid_seqs, b_tg_trks, b_labels, _ in wsd_data['dev'].make_batches(32, shuffle_data=True):\n",
    "        b_tid_seqs = torch.tensor(b_tid_seqs, device=classifier.device)\n",
    "        b_tg_trks = torch.tensor(b_tg_trks, device=classifier.device)\n",
    "        b_labels = torch.tensor(b_labels, device=classifier.device).to(classifier.device)\n",
    "        print('input size : ',b_tid_seqs.size(),\" reference size : \",b_tg_trks.size())\n",
    "        log_probs = classifier(b_tid_seqs, b_tg_trks)\n",
    "        print('output size : ',log_probs.size(),\" reference size : \",b_labels.size())\n",
    "       \n",
    "        gold = b_labels[0] #.item()\n",
    "        print(\"GOLD LABEL of first ex %d ( = %s)\" % (gold, i2label[gold]))\n",
    "        print(\"LOG_PROBS before training: %s\\n\\n\" % str(log_probs[0]))\n",
    "        break\n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v2Hz7e9-oZcF",
    "outputId": "c007685c-8da8-449c-d5c8-89023191b9b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16792,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(wsd_data['train'].labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etG6nI8CofMp",
    "outputId": "62495e87-c285-4114-9e4d-7b9ef9397b65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(label2i.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aikfNi0zexy9"
   },
   "source": [
    "### Entraînement : fine-tuning sur la tâche de WSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9p28nDcdYUqi",
    "outputId": "e681959b-5fef-45cb-ae26-471bb53ba15d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.....\n",
      "Training..... epoch nr:  0\n",
      "--------\n",
      "train loss:  1.8969914008909305 val accuracy:  0.7514712806026366\n",
      "--------\n",
      "Training..... epoch nr:  1\n",
      "--------\n",
      "train loss:  1.4174101578192113 val accuracy:  0.7999058380414312\n",
      "--------\n",
      "Training..... epoch nr:  2\n",
      "--------\n",
      "train loss:  1.1947084778993 val accuracy:  0.8171492467043314\n",
      "--------\n",
      "Training..... epoch nr:  3\n",
      "--------\n",
      "train loss:  1.0610616947364897 val accuracy:  0.8237405838041432\n",
      "--------\n",
      "Training..... epoch nr:  4\n",
      "--------\n",
      "train loss:  0.969521684610345 val accuracy:  0.8311558380414312\n",
      "--------\n",
      "Training..... epoch nr:  5\n",
      "--------\n",
      "train loss:  0.9033053362097577 val accuracy:  0.83674670433145\n",
      "--------\n",
      "Training..... epoch nr:  6\n",
      "--------\n",
      "train loss:  0.8533898721985814 val accuracy:  0.8309204331450094\n",
      "--------\n",
      "Training..... epoch nr:  7\n",
      "--------\n",
      "train loss:  0.8133916622488104 val accuracy:  0.8364524482109228\n",
      "--------\n",
      "Training..... epoch nr:  8\n",
      "--------\n",
      "train loss:  0.7814886118724067 val accuracy:  0.8372763653483992\n",
      "--------\n",
      "Training..... epoch nr:  9\n",
      "--------\n",
      "train loss:  0.7549222536413388 val accuracy:  0.8449270244821092\n",
      "--------\n",
      "Training..... epoch nr:  10\n",
      "--------\n",
      "train loss:  0.7318554399615541 val accuracy:  0.8452212806026366\n",
      "--------\n",
      "Training..... epoch nr:  11\n",
      "--------\n",
      "train loss:  0.7122188170307112 val accuracy:  0.8457509416195856\n",
      "--------\n",
      "Training..... epoch nr:  12\n",
      "--------\n",
      "train loss:  0.6948119650182755 val accuracy:  0.8351577212806026\n",
      "--------\n",
      "Training..... epoch nr:  13\n",
      "--------\n",
      "train loss:  0.6793232508436744 val accuracy:  0.8386299435028248\n",
      "--------\n",
      "Training..... epoch nr:  14\n",
      "--------\n",
      "train loss:  0.6655975258131294 val accuracy:  0.8441619585687382\n",
      "--------\n",
      "Training..... epoch nr:  15\n",
      "--------\n",
      "train loss:  0.6532883511417096 val accuracy:  0.843632297551789\n",
      "--------\n",
      "Stopping early...\n",
      "train losses: 1.8970 / 1.4174 / 1.1947 / 1.0611 / 0.9695 / 0.9033 / 0.8534 / 0.8134 / 0.7815 / 0.7549 / 0.7319 / 0.7122 / 0.6948 / 0.6793 / 0.6656 / 0.6533\n",
      "val   losses: 0.4446 / 0.4906 / 0.4717 / 0.5837 / 0.4165 / 0.5602 / 0.4175 / 0.8654 / 0.3548 / 0.4017 / 0.8088 / 0.1505 / 0.3245 / 0.2899 / 0.2127 / 0.8245 / 0.2430 / 0.5529 / 0.3536 / 0.7460 / 0.3997 / 0.4919 / 0.3113 / 0.2902 / 0.1112 / 0.3324 / 0.2541 / 0.2787 / 0.6638 / 0.6803 / 1.0220 / 0.5547 / 0.4437 / 0.7062 / 0.6365 / 0.2758 / 0.6998 / 0.3265 / 0.3097 / 0.1782 / 0.4985 / 0.6128 / 0.6053 / 0.7657 / 0.3045 / 0.7362 / 0.3088 / 0.3082 / 0.9605 / 0.7358 / 0.6740 / 0.4807 / 0.5570 / 0.5817 / 0.4050 / 1.1479 / 0.5645 / 1.0434 / 1.4740\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0005\n",
    "n_epochs = 20\n",
    "import numpy as np\n",
    "stop_early = 0\n",
    "early_stopping_patience = 6\n",
    "\n",
    "\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.NLLLoss() \n",
    "#optimizer = optim.SGD(classifier.parameters(), lr=LR)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=LR)\n",
    "\n",
    "config_name = 'sequoiaftb.asfalda_1_3.wsd.lr' + 'Adam' + str(LR) + '_bs' + str(BATCH_SIZE)\n",
    "out_model_file = './' + config_name + '.model'\n",
    "out_log_file = './' + config_name + '.log'\n",
    "\n",
    "\n",
    "# perte à chaque époque (sur le train / sur le validation set)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "min_val_loss = None\n",
    "epoch_losses = []\n",
    "# pour tests: utiliser training sur dev\n",
    "#train_data = data['dev'] # data['train']\n",
    "train_data = wsd_data['train']\n",
    "val_data = wsd_data['val']\n",
    "dev_data = wsd_data['dev']\n",
    "classifier.train()\n",
    "early_stopping_patience = 0 #how many times dev accuracy might be smaller than previous time\n",
    "stop_early = 6\n",
    "\n",
    "print('Training.....')\n",
    "acc = 0\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "  classifier.train()\n",
    "  \n",
    "  print('Training..... epoch nr: ', epoch)\n",
    "  for b_tid_seqs, b_tg_trks, b_labels, _ in train_data.make_batches(64, shuffle_data=True):\n",
    "    \n",
    "  \n",
    "    optimizer.zero_grad()\n",
    "    b_tid_seqs = torch.tensor(b_tid_seqs, device=classifier.device).to(classifier.device)\n",
    "    b_tg_trks = torch.tensor(b_tg_trks, device=classifier.device).to(classifier.device)\n",
    "    b_labels = torch.tensor(b_labels, device=classifier.device).to(classifier.device)\n",
    "    log_probs = classifier(b_tid_seqs, b_tg_trks).to(classifier.device)\n",
    "  \n",
    "    loss = loss_function(log_probs,  b_labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "  epoch_losses.append(sum(train_losses)/len(train_losses))\n",
    "  \n",
    "  pred_labels, val_losses, val_acc, _, _ = classifier.run_on_dataset(val_data, optimizer, batch_size=32, validation_use=True)\n",
    "  print('--------')\n",
    "  print('train loss: ',epoch_losses[-1],'val accuracy: ', val_acc)\n",
    "  print('--------')\n",
    "  if val_acc>=acc:\n",
    "    acc=val_acc\n",
    "  else:\n",
    "    early_stopping_patience +=1\n",
    "    if early_stopping_patience == stop_early:\n",
    "      print('Stopping early...')\n",
    "      break\n",
    "\n",
    "  \n",
    "    \n",
    "  \n",
    " \n",
    "\n",
    "\n",
    "        \n",
    "  \n",
    "           \n",
    "# TODO HERE\n",
    "# training\n",
    "# - basic : train for NB_EPOCHS +\n",
    "# - BONUS : early stopping: stop epoch loop as soon as accuracy on dev decreases+\n",
    "\n",
    "# don't forget to toggle \n",
    "# - classifier.train() when training on train\n",
    "# - classifier.eval() when evaluating on val corpus\n",
    "\n",
    "\n",
    "print(\"train losses: %s\" % ' / '.join([ \"%.4f\" % x for x in epoch_losses]))\n",
    "print(\"val   losses: %s\" % ' / '.join([ \"%.4f\" % x for x in val_losses]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOvw3cXaYKtz"
   },
   "source": [
    "Below is a version of classifier with added weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eOKrCRPFexy-",
    "outputId": "3d5ab60b-142b-4436-c5e9-9831617ee55c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at flaubert/flaubert_base_cased were not used when initializing FlaubertModel: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "- This IS expected if you are initializing FlaubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compute_class_weight() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [81], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m   get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install scikit-learn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m class_weight \u001b[38;5;241m=\u001b[39m compute_class_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39munique(\u001b[38;5;28mlist\u001b[39m(label2i\u001b[38;5;241m.\u001b[39mvalues())),\u001b[38;5;28mlist\u001b[39m(label2i\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m     20\u001b[0m class_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(class_weight, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(classifier\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#loss_function = nn.CrossEntropyLoss()\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: compute_class_weight() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "# training\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "stop_early = 0\n",
    "early_stopping_patience = 6\n",
    "\n",
    "\n",
    "classifier_weights = WSDClassifier(num_labels, device = 'cuda', use_mlp = False, hidden_size = 100,nbr_lemmas = len(lemma2i), lemma_embedding_size = 518, add_lemmas = False)\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0005\n",
    "n_epochs = 20\n",
    "import numpy as np\n",
    "try:\n",
    "  from sklearn.utils.class_weight import compute_class_weight\n",
    "except ImportError:\n",
    "  !pip install scikit-learn\n",
    "\n",
    "class_weight = compute_class_weight(\"balanced\", np.unique(list(label2i.values())),list(label2i.values()))\n",
    "\n",
    "class_weight = torch.tensor(class_weight, dtype=torch.float).to(classifier.device)\n",
    "\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.NLLLoss(weight = class_weight) \n",
    "#optimizer = optim.SGD(classifier.parameters(), lr=LR)\n",
    "optimizer = optim.Adam(classifier_weights.parameters(), lr=LR)\n",
    "\n",
    "config_name = 'sequoiaftb.asfalda_1_3.wsd.lr' + 'Adam' + str(LR) + '_bs' + str(BATCH_SIZE)\n",
    "out_model_file = './' + config_name + '.model'\n",
    "out_log_file = './' + config_name + '.log'\n",
    "\n",
    "\n",
    "# perte à chaque époque (sur le train / sur le validation set)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "min_val_loss = None\n",
    "epoch_losses = []\n",
    "# pour tests: utiliser training sur dev\n",
    "#train_data = data['dev'] # data['train']\n",
    "train_data = wsd_data['train']\n",
    "val_data = wsd_data['val']\n",
    "dev_data = wsd_data['dev']\n",
    "classifier_weights.train()\n",
    "early_stopping_patience = 0 #how many times dev accuracy might be smaller than previous time\n",
    "stop_early = 6\n",
    "\n",
    "print('Training.....')\n",
    "acc = 0\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "  classifier_weights.train()\n",
    "  \n",
    "  print('Training..... epoch nr: ', epoch)\n",
    "  for b_tid_seqs, b_tg_trks, b_labels, _ in train_data.make_batches(64, shuffle_data=True):\n",
    "    \n",
    "  \n",
    "    optimizer.zero_grad()\n",
    "    b_tid_seqs = torch.tensor(b_tid_seqs, device=classifier_weights.device).to(classifier_weights.device)\n",
    "    b_tg_trks = torch.tensor(b_tg_trks, device=classifier_weights.device).to(classifier_weights.device)\n",
    "    b_labels = torch.tensor(b_labels, device=classifier_weights.device).to(classifier_weights.device)\n",
    "    log_probs = classifier_weights(b_tid_seqs, b_tg_trks).to(classifier_weights.device)\n",
    "  \n",
    "    loss = loss_function(log_probs,  b_labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "  epoch_losses.append(sum(train_losses)/len(train_losses))\n",
    " \n",
    "  \n",
    "  pred_labels, val_losses, val_acc, _, _ = classifier_weights.run_on_dataset(val_data, optimizer, batch_size=32, validation_use=True)\n",
    "  val_accs.append(val_acc)\n",
    "  print('--------')\n",
    "  print('train loss: ',epoch_losses[-1],'val accuracy: ', val_acc)\n",
    "  print('--------')\n",
    " \n",
    "  if val_acc>=acc:\n",
    "    acc=val_acc\n",
    "  else:\n",
    "    early_stopping_patience +=1\n",
    "    if early_stopping_patience == stop_early:\n",
    "      print('Stopping early...')\n",
    "      break\n",
    "\n",
    "\n",
    "# TODO HERE\n",
    "# training\n",
    "# - basic : train for NB_EPOCHS +\n",
    "# - BONUS : early stopping: stop epoch loop as soon as accuracy on dev decreases+\n",
    "\n",
    "# don't forget to toggle \n",
    "# - classifier.train() when training on train\n",
    "# - classifier.eval() when evaluating on val corpus\n",
    "\n",
    "\n",
    "print(\"train losses: %s\" % ' / '.join([ \"%.4f\" % x for x in epoch_losses]))\n",
    "print(\"val   losses: %s\" % ' / '.join([ \"%.4f\" % x for x in val_losses]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "LIXDclLpntmM",
    "outputId": "7a67fa45-3617-4ee4-a3b1-24204c586160"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epoch_losses)\n",
    "plt.ylabel(n_epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "gbiznzHfntmM",
    "outputId": "6e37ceaf-8bef-44ba-f7de-576e8227cacc"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(val_accs)\n",
    "plt.ylabel(n_epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZ3Hr8WplthG"
   },
   "source": [
    "We then added option lemmas, MLP and weights to balance weights.\n",
    "The results are seen below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jjSKO4orlYbQ",
    "outputId": "a93cded2-9ec1-438d-95c9-3fa9da2d6772"
   },
   "outputs": [],
   "source": [
    "classifier_mlp_weights_lemmas = WSDClassifier(num_labels, device = 'cuda', use_mlp = True, hidden_size = 100,nbr_lemmas = len(lemma2i), lemma_embedding_size = 518, add_lemmas = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4s6pTO8_m02p"
   },
   "outputs": [],
   "source": [
    "#for name, param in classifier.named_parameters():\n",
    "  #print(\"PARAM named %s, of shape %s\" % (name, str(param.shape)))\n",
    "  #print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3nQUTyr5l4fj",
    "outputId": "e930755d-9dd1-4776-f3d7-3fdff97cdee6"
   },
   "outputs": [],
   "source": [
    "# training\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "stop_early = 0\n",
    "early_stopping_patience = 6\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0005\n",
    "n_epochs = 30\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weight = compute_class_weight(\"balanced\", np.unique(list(label2i.values())),list(label2i.values()))\n",
    "\n",
    "class_weight = torch.tensor(class_weight, dtype=torch.float).to(classifier.device)\n",
    "\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.NLLLoss(weight = class_weight) \n",
    "#optimizer = optim.SGD(classifier.parameters(), lr=LR)\n",
    "optimizer = optim.Adam(classifier_mlp_weights_lemmas.parameters(), lr=LR)\n",
    "\n",
    "config_name = 'sequoiaftb.asfalda_1_3.wsd.lr' + 'Adam' + str(LR) + '_bs' + str(BATCH_SIZE)\n",
    "out_model_file = './' + config_name + '.model'\n",
    "out_log_file = './' + config_name + '.log'\n",
    "\n",
    "\n",
    "# perte à chaque époque (sur le train / sur le validation set)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "min_val_loss = None\n",
    "epoch_losses = []\n",
    "# pour tests: utiliser training sur dev\n",
    "#train_data = data['dev'] # data['train']\n",
    "train_data = wsd_data['train']\n",
    "val_data = wsd_data['val']\n",
    "dev_data = wsd_data['dev']\n",
    "\n",
    "val_accs = []\n",
    "\n",
    "print('Training.....')\n",
    "acc = 0\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "  classifier_mlp_weights_lemmas.train()\n",
    "  \n",
    "  print('acc',acc)\n",
    "  print('Training..... epoch nr: ', epoch)\n",
    "  for b_tid_seqs, b_tg_trks, b_labels, b_lemma_idx in train_data.make_batches(64, shuffle_data=True):\n",
    "    \n",
    "  \n",
    "    optimizer.zero_grad()\n",
    "    b_tid_seqs = torch.tensor(b_tid_seqs, device=classifier_mlp_weights_lemmas.device).to(classifier_mlp_weights_lemmas.device)\n",
    "    b_tg_trks = torch.tensor(b_tg_trks, device=classifier_mlp_weights_lemmas.device).to(classifier_mlp_weights_lemmas.device)\n",
    "    b_labels = torch.tensor(b_labels, device=classifier_mlp_weights_lemmas.device).to(classifier_mlp_weights_lemmas.device)\n",
    "    b_lemma_idx = torch.tensor(b_lemma_idx, device=classifier_mlp_weights_lemmas.device).to(classifier_mlp_weights_lemmas.device)\n",
    "    log_probs = classifier_mlp_weights_lemmas(b_tid_seqs, b_tg_trks, b_lemma_idx).to(classifier_mlp_weights_lemmas.device)\n",
    "  \n",
    "    loss = loss_function(log_probs,  b_labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "  epoch_losses.append(sum(train_losses)/len(train_losses))\n",
    " \n",
    "  pred_labels, val_losses, val_acc, _, _ = classifier_mlp_weights_lemmas.run_on_dataset(val_data, optimizer, batch_size=32, validation_use=True)\n",
    "  val_accs.append(val_acc)\n",
    "  print('--------')\n",
    "  print('train loss: ',epoch_losses[-1],'val accuracy: ', val_acc)\n",
    "  print('--------')\n",
    " \n",
    "  if val_acc>=acc:\n",
    "    acc=val_acc\n",
    "  else:\n",
    "    early_stopping_patience +=1\n",
    "    if early_stopping_patience == stop_early:\n",
    "      print('Stopping early...')\n",
    "      break\n",
    "\n",
    "\n",
    "           \n",
    "# TODO HERE\n",
    "# training\n",
    "# - basic : train for NB_EPOCHS +\n",
    "# - BONUS : early stopping: stop epoch loop as soon as accuracy on dev decreases+\n",
    "\n",
    "# don't forget to toggle \n",
    "# - classifier.train() when training on train\n",
    "# - classifier.eval() when evaluating on val corpus\n",
    "\n",
    "\n",
    "print(\"train losses: %s\" % ' / '.join([ \"%.4f\" % x for x in epoch_losses]))\n",
    "print(\"val   losses: %s\" % ' / '.join([ \"%.4f\" % x for x in val_losses]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "L7KhwZHXvnQc",
    "outputId": "b420b714-840c-431a-9e3e-13dd68779b0b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epoch_losses)\n",
    "plt.ylabel(n_epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "O8v2EgUGvo_m",
    "outputId": "d141ae2e-807d-4113-c039-27bf142127b5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(val_accs)\n",
    "plt.ylabel(n_epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsgc93t3XPkN"
   },
   "source": [
    "Below is a variant of classifier using class weight and MLP layer but NOT lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GugrUBWavbnB"
   },
   "outputs": [],
   "source": [
    "classifier_mlp_weights = WSDClassifier(num_labels, device = 'cuda', use_mlp = True, hidden_size = 100,nbr_lemmas = len(lemma2i), lemma_embedding_size = 518, add_lemmas = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Tqpu49MvgrG",
    "outputId": "ecdc6a74-5dfe-4e05-d446-ce07d6546b26"
   },
   "outputs": [],
   "source": [
    "# training\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "stop_early = 0\n",
    "early_stopping_patience = 6\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.0005\n",
    "n_epochs = 25\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weight = compute_class_weight(\"balanced\", np.unique(list(label2i.values())),list(label2i.values()))\n",
    "\n",
    "class_weight = torch.tensor(class_weight, dtype=torch.float).to(classifier.device)\n",
    "\n",
    "#loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.NLLLoss(weight = class_weight) \n",
    "#optimizer = optim.SGD(classifier_mlp_weights.parameters(), lr=LR)\n",
    "optimizer = optim.Adam(classifier_mlp_weights.parameters(), lr=LR)\n",
    "\n",
    "config_name = 'sequoiaftb.asfalda_1_3.wsd.lr' + 'Adam' + str(LR) + '_bs' + str(BATCH_SIZE)\n",
    "out_model_file = './' + config_name + '.model'\n",
    "out_log_file = './' + config_name + '.log'\n",
    "\n",
    "\n",
    "# perte à chaque époque (sur le train / sur le validation set)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "min_val_loss = None\n",
    "epoch_losses = []\n",
    "# pour tests: utiliser training sur dev\n",
    "#train_data = data['dev'] # data['train']\n",
    "train_data = wsd_data['train']\n",
    "val_data = wsd_data['val']\n",
    "dev_data = wsd_data['dev']\n",
    "classifier_mlp_weights.train()\n",
    "\n",
    "print('Training.....')\n",
    "acc = 0\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "  classifier_mlp_weights.train()\n",
    "  \n",
    "  print('acc',acc)\n",
    "  print('Training..... epoch nr: ', epoch)\n",
    "  for b_tid_seqs, b_tg_trks, b_labels, b_lemma_idx in train_data.make_batches(64, shuffle_data=True):\n",
    "    \n",
    "  \n",
    "    optimizer.zero_grad()\n",
    "    b_tid_seqs = torch.tensor(b_tid_seqs, device=classifier_mlp_weights.device).to(classifier_mlp_weights.device)\n",
    "    b_tg_trks = torch.tensor(b_tg_trks, device=classifier_mlp_weights.device).to(classifier_mlp_weights.device)\n",
    "    b_labels = torch.tensor(b_labels, device=classifier_mlp_weights.device).to(classifier_mlp_weights.device)\n",
    "    b_lemma_idx = torch.tensor(b_lemma_idx, device=classifier_mlp_weights.device).to(classifier_mlp_weights.device)\n",
    "    log_probs = classifier_mlp_weights(b_tid_seqs, b_tg_trks, b_lemma_idx).to(classifier_mlp_weights.device)\n",
    "  \n",
    "    loss = loss_function(log_probs,  b_labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "  epoch_losses.append(sum(train_losses)/len(train_losses))\n",
    "  \n",
    "  pred_labels, val_losses, val_acc, _, _ =classifier_mlp_weights.run_on_dataset(val_data, optimizer, batch_size=32, validation_use=True)\n",
    "  val_accs.append(val_acc)\n",
    "  print('--------')\n",
    "  print('train loss: ',epoch_losses[-1],'val accuracy: ', val_acc)\n",
    "  print('--------')\n",
    " \n",
    "  if val_acc>=acc:\n",
    "    acc=val_acc\n",
    "  else:\n",
    "    early_stopping_patience +=1\n",
    "    if early_stopping_patience == stop_early:\n",
    "      print('Stopping early...')\n",
    "      break\n",
    "\n",
    "  \n",
    "    \n",
    "  \n",
    " \n",
    "\n",
    "\n",
    "        \n",
    "  \n",
    "           \n",
    "# TODO HERE\n",
    "# training\n",
    "# - basic : train for NB_EPOCHS +\n",
    "# - BONUS : early stopping: stop epoch loop as soon as accuracy on dev decreases+\n",
    "\n",
    "# don't forget to toggle \n",
    "# - classifier.train() when training on train\n",
    "# - classifier.eval() when evaluating on val corpus\n",
    "\n",
    "\n",
    "print(\"train losses: %s\" % ' / '.join([ \"%.4f\" % x for x in epoch_losses]))\n",
    "print(\"val   losses: %s\" % ' / '.join([ \"%.4f\" % x for x in val_losses]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "iICXMb1IwDbZ",
    "outputId": "8b1373c2-0905-4dfe-c329-0f6238b0ecda"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epoch_losses)\n",
    "plt.ylabel(n_epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "F93P9OlMwDbk",
    "outputId": "f5e8444c-c0f9-4c1c-b63c-70d974557239"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(val_accs)\n",
    "plt.ylabel(n_epochs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHt6rTougG0K"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYvpk9VHb4CD"
   },
   "outputs": [],
   "source": [
    "test_data = wsd_data['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XjSdDIGTf3a"
   },
   "source": [
    "**Normal version of classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHYxcJBNexzB",
    "outputId": "f88194d0-d169-4a75-ac61-b70f6246aafd"
   },
   "outputs": [],
   "source": [
    "# TODO HERE : run on dev and evaluate\n",
    "pred_labels, val_losses, dev_acc, b_labels, _ = classifier.run_on_dataset(dev_data, optimizer, batch_size=32, validation_use=False)\n",
    "pred_labels, val_losses, test_acc, b_labels, _ = classifier.run_on_dataset(test_data, optimizer, batch_size=32, validation_use=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VHBqVMXT0WW",
    "outputId": "1e0c7e43-1c79-4ebb-fbe4-501abb5dee69"
   },
   "outputs": [],
   "source": [
    "print('dev acc: ', dev_acc, 'test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZH92iHUaTjGA"
   },
   "source": [
    "**Classifier with mlp, lemmas and weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unUsvJIJb_M5"
   },
   "outputs": [],
   "source": [
    "pred_labels, val_losses, dev_acc1, b_labels, _ = classifier_mlp_weights_lemmas.run_on_dataset(dev_data, optimizer, batch_size=32, validation_use=False)\n",
    "pred_labels, val_losses, test_acc1, b_labels, _ = classifier_mlp_weights_lemmas.run_on_dataset(test_data, optimizer, batch_size=32, validation_use=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6RvXGnuT9qE",
    "outputId": "1651c3d4-2b10-47b8-b221-0fcceac76a00"
   },
   "outputs": [],
   "source": [
    "print('dev acc: ', dev_acc1, 'test acc:', test_acc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58FEY1qRTnlh"
   },
   "source": [
    "**Classifier with weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ex411WDicACm",
    "outputId": "e6e32df1-9148-4628-ca28-9b7c02436623"
   },
   "outputs": [],
   "source": [
    "pred_labels, val_losses, dev_acc2, b_labels, _ = classifier_weights.run_on_dataset(dev_data, optimizer, batch_size=32, validation_use=False)\n",
    "pred_labels, val_losses, test_acc2, b_labels, _ = classifier_weights.run_on_dataset(test_data, optimizer, batch_size=32, validation_use=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fGjo_stpUAT7",
    "outputId": "135a38a9-5ca7-4ec9-a8b9-502f739b1e7a"
   },
   "outputs": [],
   "source": [
    "print('dev acc: ', dev_acc2, 'test acc:' , test_acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KzKpherTqsj"
   },
   "source": [
    "**Classifier mlp and weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7tpF6H5cArk",
    "outputId": "ae365b05-0e8f-420e-90ff-bb064f79109a"
   },
   "outputs": [],
   "source": [
    "pred_labels, val_losses, dev_acc3, b_labels, _ = classifier_mlp_weights.run_on_dataset(dev_data, optimizer, batch_size=32, validation_use=False)\n",
    "pred_labels, val_losses, test_acc3, b_labels, _ = classifier_mlp_weights.run_on_dataset(test_data, optimizer, batch_size=32, validation_use=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zwrX4nJoUCKS",
    "outputId": "2b07eccf-5fc6-4bdf-bf53-9d253a48c823"
   },
   "outputs": [],
   "source": [
    "print('dev acc: ', dev_acc3, 'test acc:', test_acc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--HB8b_iWmU_"
   },
   "source": [
    "**I solved the problem of UserWarning, it's still present in the output cells because I didn't want to retrain everything**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmIAkHHqSbXZ"
   },
   "outputs": [],
   "source": [
    "# BONUS : generalization analysis\n",
    "#   Do you think it would be better to predict seen-in-train lemma/frame associations only ?\n",
    "#   (implement analysis of the predictions to answer that question)\n",
    "\n",
    "# VARIOUS OTHER POSSIBLE BONUSES: does it help to:\n",
    "# - fine-tune with a MLP instead of single layer ? + ADDED\n",
    "# - balance classes (\"Other_sense\" is over represented in dataset) ?+ ADDED\n",
    "#    Not sure, because natural distribution of data is a precious clue (cf. MFS)\n",
    "# - add a lemma embedding of the target ? - nn_embeddings ? + ADDED\n",
    "# - use the average of the target subword tokens instead of the first one only ?\n",
    "# ... other ideas are welcome ...\n",
    "#ADDED also early stopping \n",
    "\n",
    "\n",
    "#@@ Très bien pour les bonus"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Pytorch 1.10.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "062aa7e54cce40c99ca2e1043f134b90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0a05f98a9e1a48729e833652dc413be5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ffee7d44a3347adbc88c47208679bd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "200efeb1a36f47b888af30721f1e8a97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a05f98a9e1a48729e833652dc413be5",
      "placeholder": "​",
      "style": "IPY_MODEL_496703edea874c6fb8a22fd32e456081",
      "value": " 896k/896k [00:00&lt;00:00, 1.58MB/s]"
     }
    },
    "21b7360121934bc392a719c6eedef583": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23f0f80a2faa4228a1897b5b92562fff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ced61c53666f45f48fe301f2af4e5e35",
      "max": 895731,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_062aa7e54cce40c99ca2e1043f134b90",
      "value": 895731
     }
    },
    "2a63baf0cdab419d985d39d07e0b376c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39f9c710ebd84b6fa81c4b9205c18a15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "496703edea874c6fb8a22fd32e456081": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "67034dd7a099431d92976a1c2d237e31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adb58c36931a412e802c295694410b9c",
      "max": 1561415,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_70315c6a722e4176a2af4b631c755d36",
      "value": 1561415
     }
    },
    "6bebeb3da1ff494196ff8b3c5beb3cff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "70315c6a722e4176a2af4b631c755d36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "719b612ae5ab417d81876ae8744cf85f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_23f0f80a2faa4228a1897b5b92562fff",
       "IPY_MODEL_200efeb1a36f47b888af30721f1e8a97"
      ],
      "layout": "IPY_MODEL_39f9c710ebd84b6fa81c4b9205c18a15"
     }
    },
    "8fd286d4947644c5b7f1cc9cb8f5b179": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a15458eba4fe4a3b93dcc3f9494990e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5611e1877c246e88f8c9ba7f73fcd14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b43699041dea4a7bb0f97ccf078e5c8b",
       "IPY_MODEL_bc909bc1b05b4067a947a03dd2708a17"
      ],
      "layout": "IPY_MODEL_2a63baf0cdab419d985d39d07e0b376c"
     }
    },
    "adb58c36931a412e802c295694410b9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b43699041dea4a7bb0f97ccf078e5c8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a15458eba4fe4a3b93dcc3f9494990e3",
      "max": 1496,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6bebeb3da1ff494196ff8b3c5beb3cff",
      "value": 1496
     }
    },
    "bc909bc1b05b4067a947a03dd2708a17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21b7360121934bc392a719c6eedef583",
      "placeholder": "​",
      "style": "IPY_MODEL_fd9fe2c7ea1c4d3b98de684243201383",
      "value": " 1.50k/1.50k [00:02&lt;00:00, 669B/s]"
     }
    },
    "cb2114aab6ab480cb6023fa0e5578c28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_67034dd7a099431d92976a1c2d237e31",
       "IPY_MODEL_ff41d7c375274c64931a86c6d8c7a0bb"
      ],
      "layout": "IPY_MODEL_ea2cbc25c2504383bbe9ecd164448d84"
     }
    },
    "ced61c53666f45f48fe301f2af4e5e35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea2cbc25c2504383bbe9ecd164448d84": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd9fe2c7ea1c4d3b98de684243201383": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff41d7c375274c64931a86c6d8c7a0bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fd286d4947644c5b7f1cc9cb8f5b179",
      "placeholder": "​",
      "style": "IPY_MODEL_1ffee7d44a3347adbc88c47208679bd6",
      "value": " 1.56M/1.56M [00:01&lt;00:00, 968kB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
