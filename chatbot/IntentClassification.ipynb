{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Intent DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AddToPlaylist' 'BookRestaurant' 'GetWeather' 'PlayMusic' 'RateBook'\n",
      " 'SearchCreativeWork' 'SearchScreeningEvent']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "intent_dataset = pd.read_csv('snipsdataset.csv')\n",
    "print(intent_dataset['intents'].unique())\n",
    "print(len(intent_dataset['intents'].unique()))\n",
    "lm_dataset = pd.DataFrame()\n",
    "lm_dataset[['text']] = intent_dataset[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Add another song to the Cita RomГЎntica playli...</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>add clem burke in my playlist Pre-Party R&amp;B Jams</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Add Live from Aragon Ballroom to Trapeo</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>add Unite and Win to my night out</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Add track to my Digster Future Hits</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        intents\n",
       "0  Add another song to the Cita RomГЎntica playli...  AddToPlaylist\n",
       "1   add clem burke in my playlist Pre-Party R&B Jams  AddToPlaylist\n",
       "2            Add Live from Aragon Ballroom to Trapeo  AddToPlaylist\n",
       "3                  add Unite and Win to my night out  AddToPlaylist\n",
       "4                Add track to my Digster Future Hits  AddToPlaylist"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split DataSet in Training Set / Testing Set in 80% / 20% (can be changed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning is all about generalizing so that your inference is right on data which is unseen. \n",
    "\n",
    "If you don't split it, you would be training on the entire data available to you. \n",
    "\n",
    "Though you may now have inferences that are right on this data. \n",
    "\n",
    "But you don't know for sure, how good would your inference be on unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_train_dataset, intent_test_dataset = train_test_split(intent_dataset, test_size=0.2, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset :  3177\n",
      "Train Dataset :  12707\n",
      "Labels :  7\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Dataset : \" ,len(intent_test_dataset))\n",
    "print(\"Train Dataset : \", len(intent_train_dataset))\n",
    "print(\"Labels : \", len(intent_train_dataset.intents.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom DNN for Text Classification (with TFIDF embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Keras Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-19 15:19:20.381225: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-19 15:19:25.563026: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-19 15:19:25.563148: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-19 15:19:26.002546: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-19 15:19:37.867575: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-19 15:19:37.868141: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-19 15:19:37.868182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, LSTM, Embedding\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if Keras uses GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Sentences from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = intent_train_dataset['text'].values\n",
    "test_sentences = intent_test_dataset['text'].values\n",
    "train_labels = intent_train_dataset['intents'].values\n",
    "test_labels = intent_test_dataset['intents'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 7\n",
    "vocab_size = 15000 \n",
    "batch_size = 1000\n",
    "top_words = 15000\n",
    "embedding_vector_length = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "x_train = tokenizer.texts_to_matrix(train_sentences, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test_sentences, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[9].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find information about the album Flipper City\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         1.15686933 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         2.42236021 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         3.34796974 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[99][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_labels)\n",
    "y_train = encoder.transform(train_labels)\n",
    "y_test = encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network is created by stacking layers — this requires two main architectural decisions:\n",
    "\n",
    "How many layers to use in the model?\n",
    "\n",
    "How many hidden units to use for each layer?\n",
    "\n",
    "\n",
    "If a model has more hidden units (a higher-dimensional representation space), and/or more layers, then the network can learn more complex representations. \n",
    "\n",
    "However, it makes the network more computationally expensive and may lead to learning unwanted patterns — patterns that improve performance on training data but not on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 50)                750050    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 50)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 357       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 7)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 750,407\n",
      "Trainable params: 750,407\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-942fad79cd11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#  \"Accuracy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "#  \"Accuracy\"\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3177/3177 [==============================] - 1s 172us/step\n",
      "Test accuracy: 0.9723009103155752\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])\n",
    " \n",
    "text_labels = encoder.classes_\n",
    "\n",
    "Result = []\n",
    "for i in range(len(intent_test_dataset)):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
    "    Result.append(predicted_label)\n",
    "    \n",
    "# Append the list of results to the test dataframe\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "intent_test_dataset['result_CustomRNN'] = Result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CNN for Text Classification (with Learned embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Keras Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Concatenate, Dense, Conv2D, Reshape, MaxPool2D, Dropout, LSTM, Embedding, Bidirectional, GlobalAveragePooling1D, CuDNNLSTM, Input, Multiply, TimeDistributed, multiply, Flatten, RepeatVector, Permute, Lambda\n",
    "from keras.optimizers import Adamax, Adam\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if Keras uses GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "#K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 7\n",
    "vocab_size = 15000\n",
    "batch_size = 1000\n",
    "top_words = 15000\n",
    "embedding_vector_length = 300\n",
    "maxlen = 30\n",
    "\n",
    "## for CNN\n",
    "filter_sizes = [2,4,6]\n",
    "num_filters = 20\n",
    "drop = 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import snowballstemmer\n",
    "#SnowballStemmer\n",
    "import nltk\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "stemmer = SnowballStemmer\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "#stop.extend(lowfreq)\n",
    "toktok = nltk.tokenize.toktok.ToktokTokenizer()\n",
    "\n",
    "intent_train_dataset['cleaned'] = intent_train_dataset['text']\n",
    "intent_train_dataset['cleaned'].replace('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~’”“′‘\\\\\\]',' ',inplace=True,regex=True)\n",
    "intent_train_dataset['cleaned'] = intent_train_dataset['cleaned'].str.lower()\n",
    "intent_train_dataset['cleaned'] = intent_train_dataset['cleaned'].apply(toktok.tokenize)\n",
    "intent_train_dataset['cleaned'] = intent_train_dataset['cleaned'].apply(lambda x: [word for word in x if word not in stop])\n",
    "intent_train_dataset['cleaned'] = intent_train_dataset['cleaned'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "intent_test_dataset['cleaned'] = intent_test_dataset['text']\n",
    "intent_test_dataset['cleaned'].replace('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~’”“′‘\\\\\\]',' ',inplace=True,regex=True)\n",
    "intent_test_dataset['cleaned'] = intent_test_dataset['cleaned'].str.lower()\n",
    "intent_test_dataset['cleaned'] = intent_test_dataset['cleaned'].apply(toktok.tokenize)\n",
    "intent_test_dataset['cleaned'] = intent_test_dataset['cleaned'].apply(lambda x: [word for word in x if word not in stop])\n",
    "intent_test_dataset['cleaned'] = intent_test_dataset['cleaned'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Sentences from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = intent_train_dataset['cleaned'].values\n",
    "test_sentences = intent_test_dataset['cleaned'].values\n",
    "train_labels = intent_train_dataset['intents'].values\n",
    "test_labels = intent_test_dataset['intents'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12707, 30)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "x_train = tokenizer.texts_to_sequences(train_sentences)\n",
    "x_test = tokenizer.texts_to_sequences(test_sentences)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_labels)\n",
    "y_train = encoder.transform(train_labels)\n",
    "y_test = encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boca/.local/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 30, 300)      4500000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 30, 300, 1)   0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 29, 1, 20)    12020       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 27, 1, 20)    24020       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 25, 1, 20)    36020       reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 20)     0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 20)     0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 20)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 3, 1, 20)     0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 60)           0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 60)           0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 7)            427         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 7)            0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,572,487\n",
      "Trainable params: 4,572,487\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 10165 samples, validate on 2542 samples\n",
      "Epoch 1/25\n",
      "10165/10165 [==============================] - 6s 633us/step - loss: 1.8342 - acc: 0.4140 - val_loss: 1.6517 - val_acc: 0.8694\n",
      "Epoch 2/25\n",
      "10165/10165 [==============================] - 6s 585us/step - loss: 1.4894 - acc: 0.7998 - val_loss: 1.2624 - val_acc: 0.9308\n",
      "Epoch 3/25\n",
      "10165/10165 [==============================] - 6s 630us/step - loss: 1.0764 - acc: 0.9011 - val_loss: 0.8268 - val_acc: 0.9496\n",
      "Epoch 4/25\n",
      "10165/10165 [==============================] - 6s 590us/step - loss: 0.6875 - acc: 0.9357 - val_loss: 0.4842 - val_acc: 0.9587\n",
      "Epoch 5/25\n",
      "10165/10165 [==============================] - 6s 593us/step - loss: 0.4164 - acc: 0.9547 - val_loss: 0.2847 - val_acc: 0.9618\n",
      "Epoch 6/25\n",
      "10165/10165 [==============================] - 6s 590us/step - loss: 0.2633 - acc: 0.9606 - val_loss: 0.1889 - val_acc: 0.9666\n",
      "Epoch 7/25\n",
      "10165/10165 [==============================] - 6s 589us/step - loss: 0.1747 - acc: 0.9724 - val_loss: 0.1422 - val_acc: 0.9701\n",
      "Epoch 8/25\n",
      "10165/10165 [==============================] - 6s 587us/step - loss: 0.1357 - acc: 0.9774 - val_loss: 0.1163 - val_acc: 0.9744\n",
      "Epoch 9/25\n",
      "10165/10165 [==============================] - 6s 584us/step - loss: 0.1017 - acc: 0.9846 - val_loss: 0.1013 - val_acc: 0.9760\n",
      "Epoch 10/25\n",
      "10165/10165 [==============================] - 6s 585us/step - loss: 0.0836 - acc: 0.9875 - val_loss: 0.0931 - val_acc: 0.9768\n",
      "Epoch 11/25\n",
      "10165/10165 [==============================] - 6s 588us/step - loss: 0.0692 - acc: 0.9889 - val_loss: 0.0872 - val_acc: 0.9776\n",
      "Epoch 12/25\n",
      "10165/10165 [==============================] - 6s 592us/step - loss: 0.0576 - acc: 0.9904 - val_loss: 0.0824 - val_acc: 0.9788\n",
      "Epoch 13/25\n",
      "10165/10165 [==============================] - 6s 594us/step - loss: 0.0505 - acc: 0.9918 - val_loss: 0.0793 - val_acc: 0.9799\n",
      "Epoch 14/25\n",
      "10165/10165 [==============================] - 6s 586us/step - loss: 0.0450 - acc: 0.9930 - val_loss: 0.0778 - val_acc: 0.9803\n",
      "Epoch 15/25\n",
      "10165/10165 [==============================] - 6s 577us/step - loss: 0.0374 - acc: 0.9945 - val_loss: 0.0777 - val_acc: 0.9799\n",
      "Epoch 16/25\n",
      "10165/10165 [==============================] - 6s 583us/step - loss: 0.0341 - acc: 0.9955 - val_loss: 0.0757 - val_acc: 0.9792\n",
      "Epoch 17/25\n",
      "10165/10165 [==============================] - 6s 556us/step - loss: 0.0321 - acc: 0.9953 - val_loss: 0.0749 - val_acc: 0.9792\n",
      "Epoch 18/25\n",
      "10165/10165 [==============================] - 6s 552us/step - loss: 0.0273 - acc: 0.9967 - val_loss: 0.0739 - val_acc: 0.9803\n",
      "Epoch 19/25\n",
      "10165/10165 [==============================] - 6s 555us/step - loss: 0.0268 - acc: 0.9959 - val_loss: 0.0752 - val_acc: 0.9799\n",
      "Epoch 20/25\n",
      "10165/10165 [==============================] - 6s 554us/step - loss: 0.0224 - acc: 0.9971 - val_loss: 0.0750 - val_acc: 0.9799\n",
      "Epoch 21/25\n",
      "10165/10165 [==============================] - 6s 557us/step - loss: 0.0194 - acc: 0.9982 - val_loss: 0.0752 - val_acc: 0.9803\n",
      "Epoch 22/25\n",
      "10165/10165 [==============================] - 6s 555us/step - loss: 0.0183 - acc: 0.9978 - val_loss: 0.0748 - val_acc: 0.9807\n",
      "Epoch 23/25\n",
      "10165/10165 [==============================] - 6s 560us/step - loss: 0.0186 - acc: 0.9972 - val_loss: 0.0746 - val_acc: 0.9815\n",
      "Epoch 24/25\n",
      "10165/10165 [==============================] - 6s 556us/step - loss: 0.0160 - acc: 0.9982 - val_loss: 0.0756 - val_acc: 0.9811\n",
      "Epoch 25/25\n",
      "10165/10165 [==============================] - 6s 559us/step - loss: 0.0170 - acc: 0.9978 - val_loss: 0.0760 - val_acc: 0.9811\n"
     ]
    }
   ],
   "source": [
    "Inputs_W = Input(shape=(maxlen,))\n",
    "Inputs_E = Embedding(vocab_size, embedding_vector_length, input_length=maxlen)(Inputs_W)\n",
    "reshape = Reshape((maxlen,embedding_vector_length,1))(Inputs_E)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_vector_length), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_vector_length), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_vector_length), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(maxlen - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(maxlen - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(maxlen - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "Dense_Label = Dense(num_labels)(dropout)\n",
    "Classifier = Activation('softmax')(Dense_Label)\n",
    "\n",
    "model = Model(input=Inputs_W, output=Classifier)\n",
    "model.summary()\n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=25,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3177/3177 [==============================] - 0s 129us/step\n",
      "Test accuracy: 0.9748190112897425\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])\n",
    " \n",
    "text_labels = encoder.classes_\n",
    "\n",
    "Result = []\n",
    "for i in range(len(intent_test_dataset)):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
    "    Result.append(predicted_label)\n",
    "    \n",
    "# Append the list of results to the test dataframe\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "intent_test_dataset['result_CustomRNN'] = Result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12710    Are any animated movies playing at Magic Johns...\n",
      "1242     I would like you to add now the hits of winter...\n",
      "2811     book a highly rated restaurant in Central Afri...\n",
      "14723     I would like to hear something from Groove Shark\n",
      "1826        Add Grey Cloudy Lies to the hip hop playlist. \n",
      "Name: text, dtype: object\n",
      "['SearchScreeningEvent', 'AddToPlaylist', 'BookRestaurant', 'PlayMusic', 'AddToPlaylist']\n"
     ]
    }
   ],
   "source": [
    "print ( intent_test_dataset[\"text\"][:5])\n",
    "\n",
    "print (Result[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom LSTM for Text Classification (with Learned Vector embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Keras Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, LSTM, Embedding, Bidirectional, GlobalAveragePooling1D, RNN\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if Keras uses GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "#K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Sentences from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = intent_train_dataset['text'].values\n",
    "test_sentences = intent_test_dataset['text'].values\n",
    "train_labels = intent_train_dataset['intents'].values\n",
    "test_labels = intent_test_dataset['intents'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 7\n",
    "vocab_size = 20000\n",
    "batch_size = 100\n",
    "lstm_size = 256\n",
    "maxlen = 50\n",
    "embedding_vector_length = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "x_train = tokenizer.texts_to_sequences(train_sentences)\n",
    "x_test = tokenizer.texts_to_sequences(test_sentences)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_labels)\n",
    "y_train = encoder.transform(train_labels)\n",
    "y_test = encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(lstm_size, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])\n",
    " \n",
    "text_labels = encoder.classes_\n",
    "\n",
    "Result = []\n",
    "for i in range(len(intent_test_dataset)):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
    "    Result.append(predicted_label)\n",
    "    \n",
    "# Append the list of results to the test dataframe\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "intent_test_dataset['result_CustomLSTM'] = Result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.14 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "2ff2a5bcff9c3ac99eefbccbdbd485039bd02e6e93cd0e258cc446c7a8f44156"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
