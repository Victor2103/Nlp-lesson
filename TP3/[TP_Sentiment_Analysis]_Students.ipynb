{"cells":[{"cell_type":"markdown","metadata":{"id":"8fKDWGQlT_hP"},"source":["Copy this notebook (File>Save a copy in Drive) and then work on your copy.\n","==\n","To send me your work: Share(top-right of the window)>Get link/copy link\n","\n","Send me an email containing the link after having\n","*   changed the link permission to Editor and\n","*   allowed anyone with the link to access the notebook.\n","\n","Merit KULDKEPP : meritkuldkepp@gmail.com\n","\n","OR\n","\n","Nicolas MERLI : merli.nicolas.0@gmail.com"]},{"cell_type":"markdown","metadata":{"id":"A2BZykSRWlSN"},"source":["Goal\n","==\n","\n","We are about to design and train a neural system to perform sentiment analysis on film reviews. More precisely, the network will have to output the probability that the input review expresses a positive opinion (overall).\n","\n","The system will be a bag-of-words model using GloVe embeddings. It will have to first average the embeddings of the words of the input review, and then send the result through a simple network that should output a probability.\n","\n","There is a lot of already written code at the beginning of the notebook. It is important that you understand it as you will have to reuse/reproduce it for future work."]},{"cell_type":"markdown","metadata":{"id":"4YowcYrkUOwG"},"source":["Loading Pytorch is important.\n","=="]},{"cell_type":"code","execution_count":1,"metadata":{"id":"e2dazbs4RArm"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'torch'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Imports Pytorch.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"]}],"source":["# Imports Pytorch.\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"qEOouXSvWiNA"},"source":["Downloading the dataset\n","==\n","The dataset we are going to use this the Large Movie Review Dataset (https://ai.stanford.edu/~amaas/data/sentiment/)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pD8AIWygRhI5"},"outputs":[],"source":["# Downloads the dataset.\n","import urllib\n","\n","tmp = urllib.request.urlretrieve(\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")\n","filename = tmp[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"097EOlPhS07G"},"outputs":[],"source":["# Extracts the dataset.\n","import tarfile\n","tar = tarfile.open(filename)\n","tar.extractall()\n","tar.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sp3XCqJ5TS73"},"outputs":[],"source":["import os # Useful library to read files and inspect directories."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"hOruARLhTU4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["aclImdb\n","[TP_Sentiment_Analysis]_Students.ipynb\n",".ipynb_checkpoints\n"]}],"source":["# Shows which files and directories are present at the root of the file system.\n","for filename in os.listdir(\".\"):\n","  print(filename)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"GqYr8dYnRtCx"},"outputs":[{"name":"stdout","output_type":"stream","text":["imdb.vocab\n","train\n","imdbEr.txt\n","README\n","test\n"]}],"source":["dataset_root = \"aclImdb\"\n","# Shows which files and directories are present at the root of the dataset directory.\n","for filename in os.listdir(dataset_root):\n","  print(filename)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"VM9B2NreR-MT"},"outputs":[{"name":"stdout","output_type":"stream","text":["2444_1.txt\n","Two houses, one street, one phone booth, one car, a girl next door, a boy next door and a zombie. This list of ingredients should suffice for a great horror movie. All you need is some blue light, ambient music and...done. Not in the hands of Dutch director van Rouveroy though! <br /><br />I like to organize \"bad movie evenings\" from time to time. The concept is really simple: get some booze, get some film-loving friends, and immerse yourself in the worst cinema can offer. For such an evening this peace of filth is one of the best. Laughs guaranteed!<br /><br />The bizarre thing is, van Rouveroy is still defending her film as if it were a great achievement. To be a witness to this you'll have to listen to the DVD's commentary track. Again: disbelieve and laughs guaranteed!\n","\n","10196_1.txt\n","As a poker enthusiast I was looking forward to seeing this movie - Especially as it had Scotty Nyugen in it.<br /><br />Basically, Scotty Nyugens short spots in this film are all it has going for it.<br /><br />The characters are unlikeable and annoying, the soundtrack is awful and the plot, well, there isn't one.<br /><br />I honestly got a headache and found myself reading the barcode number on the DVD box after twenty minutes I was THAT bored. Its actually ashame that Nyugen was in this movie as otherwise I wouldn't have wasted $16 buying it off Ebay.<br /><br />Take it from me - AVOID like 7 2 offsuit!!! Dire. :(\n","\n","5690_2.txt\n","First off... I never considered myself an Uwe Boll Hater since I think I never even saw one of his movies but after seeing this cheap excuse for a movie named \"Seed\" (which is the name of the serial killer this movie is about) I am close to joining the hate club. This movie makes absolutely no sense at all... the plot is a joke and although Boll clearly tries to get attention by shocking people 90% of this movie is just plain boredom. You can sum up this movie like this: <br /><br />1. Hooded killer watches clips of animals getting tortured on TV. This is real life footage from pelt farms and the movie opens with the ridiculous reason of \"making a statement about humanity\" and giving a Peta address. Since this movie has no message at all and is the worst piece of torture porn-exploitation you already have a reason to hate the movie from the beginning onward.<br /><br />2. Death by electrocution with a pretext that gives away what happens later in this movie printed on screen so every retard gets it.<br /><br />3. Cops watch videos of animals, babies and women starved to death and decomposing in Seeds basement, having stupid nightmares and crying into their whiskey because Seed is such an evil bad mofo. Although the acting is OK the movie takes a dive every time it tries to incorporate any emotions... <br /><br />4. Cops bust Seed in his house, act stupid and get slashed in the dark. This sequence reminds me of a video game, you barely see anything except flashlights. Seed is a super killer that is everywhere at once and all cops act stupid enough to be killed... except for one who busts him.<br /><br />5. Seed gets the chair and we see his electrocution as lengthy as everything else in this \"movie\"... he won't die and we are reminded of the opening statement that he must be set free if he survives 3 electric jolts. Guess what... they just bury him alive to solve the problem.<br /><br />6. Seed comes out of his grave, kills everyone off in another slashing part and then seeks the main cop to take revenge on.<br /><br />7. A woman gets her head bashed in with a hammer in an endless sequence from one point of view just for the fun and shock value of it. <br /><br />8. Seed captures the cops family, lures him to his house, threatens to kill his wife and daughter. After killing his wife with a nail gun the cop shoots himself in the head considering thats whats Seed wants (its hard to get into that guys head since he not just wears his mask even in prison but also never utters a word ... the movie has barely any dialog anyway so don't mind).<br /><br />9. Boll goes for a nihilistic shocker end where Seed locks the daughter in with her dead dad to rot like the persons we saw on video on sequence 3.<br /><br />This is it... no message, no plot, no reason, no face behind the mask, no background except a stupid story that Seed was burnt as a child.<br /><br />This movie relies purely on few key scenes and their shock value. I hardly remember a movie this empty of any emotion or message or entertainment. Its like watching August Underground ... thats fine with me, some people will enjoy this brainless snuff. But what is really hard to stand about it is the pseudo-message in the beginning and the fact that the movie is well made considering camera-work, effects and even the acting is too good for this waste of celluloid. <br /><br />So how does Boll get money to make such \"movies\" when thousands of talented directors work on shoestring budgets?? \"Seed\" is not just the essence of ridiculous, its living proof that the free market is flawed ... lucky Uwe that the German taxpayer is paying for a lot of this waste to get deductments.\n","\n","6747_1.txt\n","Wow...speechless as to the making of this film, I can't say much. The coverbox at the local videostore should've said it all...nothing but 6 actors/actresses who get lost on the set of Scream and decide to shoot a movie!<br /><br />The acting was apparently not in the budget, but they were able to afford nudity and good-looking actors! Style over substance almost makes its mark here, except most of these acting-class failures keep forgetting that there is a plot that needs to go somewhere when they were reading this script. After only 4 or 5 kills by the so-called masked murderer and a confusing tie-in plot about a Murder Club which the dumb lead actress thinks is a real club that she can join (only if she can get over a girl bumping into her car), you want to stab your hands with the nearest sharp object to remind yourself never to get overly excited by a possibly good movie such as this.<br /><br />I feel bad for the people who bought this film and can't find anyone to take it off their hands. Another example of what's wrong with the growing number of straight to video horror releases with no thought put into the essentials. Throw it away if you did buy this.\n","\n","1791_2.txt\n","What's with the murky video in the beginning and sporadically throughout the movie? It's like someone put muddy water on the camera lens.<br /><br />The violence and nudity might turn some people off but, that, along with the mostly bad acting is what makes a good cult movie I suppose.<br /><br />My favorite line is delivered by Tarquin the Vampire, \"Alas, your breed is dumb.\" Okay, no one should ever say \"alas\" in a movie line unless they're English and living in the 18th century.<br /><br />The acting by the Van Helsing character and bad girl \"Rally\" isn't bad. I also liked Master Little played by Ron Little. Wicked martial arts! Don't take it too seriously and you'll enjoy it.\n","\n"]}],"source":["# Shows several reviews.\n","dirname = os.path.join(dataset_root, \"train\", \"neg\") # \"aclImdb/{train|test}/{neg|pos}\"\n","for idx, filename in enumerate(os.listdir(dirname)):\n","  if(idx >= 5): break # Stops after the 5th file.\n","  \n","  print(filename)\n","  with open(os.path.join(dirname, filename)) as f:\n","    review = f.read()\n","    print(review)\n","  print()"]},{"cell_type":"markdown","metadata":{"id":"ikcb0jJiaotG"},"source":["Preprocessing the dataset\n","=="]},{"cell_type":"code","execution_count":4,"metadata":{"id":"SfsLxzRGctTt"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'nltk'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m \u001b[39m# Imports NLTK, an NLP library.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mpunkt\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# Loads a module required for tokenization.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39m# This library defines useful data structures. \u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"]}],"source":["import nltk # Imports NLTK, an NLP library.\n","nltk.download('punkt') # Loads a module required for tokenization.\n","\n","import collections # This library defines useful data structures. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVCLdoQ_b_pN"},"outputs":[],"source":["# TODO\n","# Write a preprocess() function that takes a review as input\n","# 1. Replaces all \"<br />\" occurrences in the review with spaces\n","# 2. Splits the review into tokens using ntlk library (check word_tokenize method documentation)\n","# 3. Lowercase all the extracted tokens\n","# Returns a list of tokens of the current review\n","\n","newline = \"<br />\"\n","def preprocess(text):\n","  text=text.replace(newline,\" \")\n","  text=text.lower()\n","  return(nltk.tokenize.punkt(text))\n","  \n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7NWWSBBYUoto"},"outputs":[],"source":["# Reads and pre-processes the reviews.\n","dataset = {\"train\": [], \"test\": []}\n","binary_classes = {\"neg\": 0, \"pos\": 1}\n","for part_name, l in dataset.items():\n","  for class_name, value in binary_classes.items():\n","    path = os.path.join(dataset_root, part_name, class_name)\n","    print(\"Processing %s...\" % path, end='');\n","    for filename in os.listdir(path):\n","        with open(os.path.join(path, filename)) as f:\n","          review_text = f.read()\n","          review_tokens = preprocess(review_text)\n","          \n","          l.append((review_tokens, value))\n","    print(\" done\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZlDG9piYiVHL"},"outputs":[],"source":["# Splits the train set into a proper train set and a development/validation set.\n","# 'dataset[\"train\"]' happens to be a list composed of a certain number of negative examples followed by the same number of positive examples.\n","# We are going to use 3/4 of the original train set as our actual train set, and 1/4 as our development set.\n","# We want to keep balanced train and development sets, i.e. for both, half of the reviews should be positive and half should be negative.\n","if(\"dev\" in dataset): print(\"This should only be run once.\")\n","else:\n","  dev_set_half_size = int((len(dataset[\"train\"]) / 4) / 2) # Half of a quarter of the training set size.\n","  dataset[\"dev\"] = dataset[\"train\"][:dev_set_half_size] + dataset[\"train\"][-dev_set_half_size:] # Takes some negative examples at the beginning and some positive ones at the end.\n","  dataset[\"train\"] = dataset[\"train\"][dev_set_half_size:-dev_set_half_size] # Removes the examples used for the development set.\n","\n","  for (part, data) in dataset.items():\n","    class_counts = collections.defaultdict(int)\n","    for (_, p) in data: class_counts[p] += 1\n","    print(f\"{part}: {class_counts}\")\n","  print(\"Train set split into train/dev.\")"]},{"cell_type":"markdown","metadata":{"id":"RfdibF5dhMIh"},"source":["Loading the word embeddings\n","==\n","We are going to use GloVe embeddings.\n","\n","All word forms with a frequency below a given threshold are going to be considered unknown forms."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pnkKPGLYxQNR"},"outputs":[],"source":["# Computes the frequency of all word forms in the train set.\n","word_counts = collections.defaultdict(int)\n","for tokens, _ in dataset[\"train\"]:\n","  for token in tokens: word_counts[token] += 1\n","\n","print(word_counts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xgw19ofeZ23K"},"outputs":[],"source":["# Builds a vocabulary containing only those words present in the train set with a frequency above a given threshold.\n","count_threshold = 4;\n","vocabulary = set()\n","for word, count in word_counts.items():\n","    if(count > count_threshold): vocabulary.add(word)\n","\n","print(vocabulary)\n","print(len(vocabulary))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mi6oZU1kph03"},"outputs":[],"source":["import zipfile\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mnhHpcuxckbK"},"outputs":[],"source":["# Returns a dictionary {word[String]: id[Integer]} and a list of Numpy arrays\n","# `data_path` is the path of the directory containing the GloVe files (if None, 'glove.6B' is used)\n","# `max_size` is the number of word embeddings read (starting from the most frequent; in the GloVe files, the words are sorted)\n","# If `vocabulary` is specified, the output vocabulary contains the intersection of `vocabulary` and the words with a defined embedding. Otherwise, all words with a defined embedding are used.\n","def get_glove(dim=50, vocabulary=None, max_size=-1, data_path=None):\n","  dimensions = set([50, 100, 200, 300]) # Available dimensions for GloVe 6B\n","  fallback_url = 'http://nlp.stanford.edu/data/glove.6B.zip' # (Remember that in GloVe 6B, words are lowercased.)\n","\n","  assert (dim in dimensions), (f'Unavailable GloVe 6B dimension: {dim}.')\n","\n","  if(data_path is None): data_path = 'glove.6B'\n","\n","  # Checks that the data is here, otherwise downloads it.\n","  if(not os.path.isdir(data_path)):\n","    #print('Directory \"%s\" does not exist. Creation.' % data_path)\n","    os.makedirs(data_path)\n","  \n","  glove_weights_file_path = os.path.join(data_path, f'glove.6B.{dim}d.txt')\n","  \n","  if(not os.path.isfile(glove_weights_file_path)):\n","    local_zip_file_path = os.path.join(data_path, os.path.basename(fallback_url))\n","  \n","    if(not os.path.isfile(local_zip_file_path)):\n","      print(f'Retreiving GloVe embeddings from {fallback_url}.')\n","      urllib.request.urlretrieve(fallback_url, local_zip_file_path)\n","    \n","    with zipfile.ZipFile(local_zip_file_path, 'r') as z:\n","      print(f'Extracting GloVe embeddings from {local_zip_file_path}.')\n","      z.extractall(path=data_path)\n","  \n","  assert os.path.isfile(glove_weights_file_path), (f\"GloVe file {glove_weights_file_path} not found.\")\n","\n","  # Reads GloVe data.\n","  print('Reading GloVe embeddings.')\n","  new_vocabulary = {} # A dictionary {word[String]: id[Integer]}\n","  embeddings = [] # The list of embeddings (Numpy arrays)\n","  with open(glove_weights_file_path, 'r') as f:\n","    for line in f: # Each line consist of the word followed by a space and all of the coefficients of the vector separated by a space.\n","      values = line.split()\n","\n","      # Here, I'm trying to detect where on the line the word ends and where the vector begins. As in some version(s) of GloVe words can contain spaces, this is not entirely trivial.\n","      vector_part = ' '.join(values[-dim:])\n","      x = line.find(vector_part)\n","      word = line[:(x - 1)]\n","\n","      if((vocabulary is not None) and (not word in vocabulary)): # If a vocabulary was specified and if the word is not it…\n","        continue # …this word is skipped.\n","\n","      new_vocabulary[word] = len(new_vocabulary)\n","      embedding = np.asarray(values[-dim:], dtype=np.float32)\n","      embeddings.append(embedding)\n","\n","      if(len(new_vocabulary) == max_size): break\n","  print('(GloVe embeddings loaded.)')\n","  print()\n","\n","  return (new_vocabulary, embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KY9nJNVlpZiL"},"outputs":[],"source":["(new_vocabulary, embeddings) = get_glove(dim=50, vocabulary=vocabulary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwaov0tWpeFZ"},"outputs":[],"source":["print(len(new_vocabulary)) # Shows the size of the vocabulary.\n","print(new_vocabulary) # Shows each word and its id."]},{"cell_type":"markdown","metadata":{"id":"dIY-HdiPhgL4"},"source":["Batch generator\n","=="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x07xfo-Gux_X"},"outputs":[],"source":["# Defines a class of objects that produce batches from the dataset.\n","class BatchGenerator:\n","  def __init__(self, dataset, vocabulary):\n","    self.dataset = dataset\n","    for part in self.dataset.values(): # Shuffles the dataset so that positive and negative examples are mixed.\n","      np.random.shuffle(part)\n","\n","    self.vocabulary = vocabulary # Dictonary {word[String]: id[Integer]}\n","    self.unknown_word_id = len(vocabulary) # Id for unknown forms\n","    self.padding_idx = len(vocabulary) + 1 # Not all reviews of a given batch will have the same length. We will \"pad\" shorter reviews with a special token id so that the batch can be represented by a matrix.\n","  \n","  def length(self, data_type='train'):\n","    return len(self.dataset[data_type])\n","\n","  # Returns a random batch.\n","  # If `subset` is an integer, only a subset of the corpus is used. This can be useful to debug the system.\n","  def get_batch(self, batch_size, data_type, subset=None):\n","    data = self.dataset[data_type] # selects the relevant portion of the dataset.\n","    \n","    max_i = len(data) if(subset is None) else min(subset, len(data))\n","    instance_ids = np.random.randint(max_i, size=batch_size) # Randomly picks some instance ids.\n","\n","    return self._ids_to_batch(data, instance_ids)\n","\n","  def _ids_to_batch(self, data, instance_ids):\n","    word_ids = [] # Will be a list of lists of word ids (Integer)\n","    polarity = [] # Will be a list of review polarities (Boolean)\n","    texts = [] # Will be a list of lists of words (String)\n","    for instance_id in instance_ids:\n","      text, p = data[instance_id]\n","\n","      word_ids.append([self.vocabulary.get(w, self.unknown_word_id) for w in text])\n","      polarity.append(p)\n","      texts.append(text)\n","    \n","    # Padding\n","    self.pad(word_ids)\n","\n","    word_ids = torch.tensor(word_ids, dtype=torch.long) # Conversion to a tensor\n","    polarity = torch.tensor(polarity, dtype=torch.bool) # Conversion to a tensor\n","\n","    return (word_ids, polarity, texts) # We don't really need `texts` but it might be useful to debug the system.\n","  \n","  # Pads a list of lists (i.e. adds fake word ids so that all sequences in the batch have the same length, so that we can use a matrix to represent them).\n","  # In place\n","  def pad(self, word_ids):\n","    max_length = max([len(s) for s in word_ids])\n","    for s in word_ids: s.extend([self.padding_idx] * (max_length - len(s)))\n","  \n","  # Returns a generator of batches for a full epoch.\n","  # If `subset` is an integer, only a subset of the corpus is used. This can be useful to debug the system.\n","  def all_batches(self, batch_size, data_type=\"train\", subset=None):\n","    data = self.dataset[data_type]\n","    \n","    max_i = len(data) if(subset is None) else min(subset, len(data))\n","\n","    # Loop that generates all full batches (batches of size 'batch_size')\n","    i = 0\n","    while((i + batch_size) <= max_i):\n","      instance_ids = np.arange(i, (i + batch_size))\n","      yield self._ids_to_batch(data, instance_ids)\n","      i += batch_size\n","    \n","    # Possibly generates the last (not full) batch.\n","    if(i < max_i):\n","      instance_ids = np.arange(i, max_i)\n","      yield self._ids_to_batch(data, instance_ids)\n","  \n","  # Turns a list of arbitrary pre-processed texts into a batch.\n","  # This function will be used to infer the polarity of a unannotated review.\n","  def turn_into_batch(self, texts):\n","    word_ids = [[self.vocabulary.get(w, self.unknown_word_id) for w in text] for text in texts]\n","    self.pad(word_ids)\n","    return torch.tensor(word_ids, dtype=torch.long)\n","\n","batch_generator = BatchGenerator(dataset=dataset, vocabulary=new_vocabulary)\n","print(batch_generator.length('train')) # Prints the number of instance in the train set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v35rEb8l0_Kd"},"outputs":[],"source":["tmp = batch_generator.get_batch(3, data_type=\"train\")\n","print(tmp[0]) # Prints the matrix of token ids.\n","print(tmp[1]) # Prints the vector of polarities.\n","print(tmp[2]) # Prints the list of reviews."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cp2VOTBzFLb9"},"outputs":[],"source":["len(list(batch_generator.all_batches(batch_size=3, data_type=\"train\"))) # Number of batches of size 3 in the training set"]},{"cell_type":"markdown","metadata":{"id":"IsTuIZoIhkTW"},"source":["The model\n","=="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PE6mbJ9Q2nUy"},"outputs":[],"source":["class SentimentClassifier(torch.nn.Module):\n","  # embeddings: list of Numpy arrays\n","  # hidden_sizes: list of the size of all hidden layers (Integer)\n","  def __init__(self, embeddings, hidden_sizes, freeze_embeddings=True, device='cuda'):\n","    embeddings = list(embeddings) # Creates a copy of the list of embeddings, so we can add or remove entries without affecting the original list.\n","    super().__init__() # Calls the constructor of the parent class. Usually, this is necessary when creating a custom module.\n","\n","    self.padding_idx = len(embeddings) + 1 # len(embeddings) will be the id of the embedding of the unknown word\n","\n","    # Here you have to (i) define a vector for unknown forms (the average of actual word embeddings) and a vector for the padding token (full of 0·s) and (ii) define an embedding layer 'self.embeddings' using torch.nn.Embedding and not forgeting to use the 'freeze' and 'padding_idx' arguments.\n","    #################\n","    \n","\n","    #################\n","    self.embeddings = self.embeddings.to(device) # Sends the word embeddings to 'device', which is potentially a GPU.\n","\n","    # Here you have to define self.main_part, the network that computes a probability out of a review (represented as the average of the embeddings of the tokens).\n","    # The number of hidden layers is determined by 'hidden_sizes, which is a list of integers describing the (output) size of each of them.\n","    # Use torch.nn.Linear to build linear layers.\n","    # torch.nn.Sequential takes one argument per module and not a list of modules as argument, but if 'modules' is a list of modules, 'torch.nn.Sequential(*modules)' (with the star notation) works.\n","    #################\n","\n","\n","    #################\n","    self.main_part = self.main_part.to(device) # Sends the network to 'device', which is potentially a GPU.\n","\n","    self.device = device\n","\n","  # 'batch' is a matrix of word ids (Integer).\n","  def forward(self, batch):\n","    # Here you have to (i) turns 'batch' into a matrix of embeddings (i.e. a tensor of rank 3), (ii) average all embeddings for a given review being careful not to take into account padding vectors, (iii) send these bag-of-words representations to the network.\n","    # Return a tensor of shape (batch size) instead of (batch size, 1).\n","    #################\n","\n","\n","    #################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZSotaQPKeO2m"},"outputs":[],"source":["# Checking if the model is well defined and if a forward pass is feasible.\n","# This cell should return a torch tensor of size 3\n","model = SentimentClassifier(embeddings, hidden_sizes=[100, 200], freeze_embeddings=True)\n","batch = batch_generator.get_batch(3, data_type=\"train\")\n","print(model(batch[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmL7aPacmNVh"},"outputs":[],"source":["# Function that computes the accuracy of the model on a given part of the dataset.\n","evaluation_batch_size = 256\n","def evaluation(data_type, subset=None):\n","  nb_correct = 0\n","  total = 0\n","  for batch in batch_generator.all_batches(batch_size, data_type=data_type, subset=subset):\n","    prob = model(batch[0].to(model.device)) # Forward pass\n","    answer = (prob > 0.5) # Shape: (batch_size, 1)\n","    nb_correct += (answer == batch[1].to(model.device)).sum().item()\n","    total += batch[0].shape[0]\n","      \n","  accuracy = (nb_correct / total)\n","  return accuracy"]},{"cell_type":"markdown","metadata":{"id":"vs_gHaFzrOaj"},"source":["Training\n","==\n","Once everything works, feel free to find better hyperparameters.\n","The goal is to maximise the accuracy on the development set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gOMCyVPK6BD8"},"outputs":[],"source":["model = embeddings, hidden_sizes=[30, 20], freeze_embeddings=False, device='cuda')\n","MSELoss = torch.nn.MSELoss()\n","\n","# Tests the model on a couple of instance before training.\n","model.eval() # Tells Pytorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n","print(model(batch_generator.turn_into_batch([preprocess(text) for text in [\"This movie was terrible!!\", \"Pure gold!\"]]).to(model.device)))\n","\n","# Training procedure\n","learning_rate = 0.006\n","l2_reg = 0.0001\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.99, weight_decay=l2_reg) # Once the backward propagation has been done, call the 'step' method (with no argument) to update the parameters.\n","batch_size = 64\n","subset = None # Use an integer to train on a smaller portion of the training set, otherwise use None.\n","epoch_size = batch_generator.length(\"train\") if(subset is None) else subset # In number of instances\n","\n","nb_epoch = 20\n","epoch_id = 0 # Id of the current epoch\n","instances_processed = 0 # Number of instances trained on in the current epoch\n","epoch_loss = [] # Will contain the loss for each batch of the current epoch\n","while(epoch_id < nb_epoch):\n","  model.train() # Tells Pytorch we are in training mode (can be useful if dropout is used, for instance).\n","  \n","  model.zero_grad() # Makes sure the gradient is reinitialised to zero.\n","  \n","  batch = batch_generator.get_batch(batch_size, data_type=\"train\", subset=subset)\n","\n","  # You have to (i) compute the prediction of the model, (ii) compute the loss (use an average over the batch), (iii) call \"backward\" on the loss and (iv) store the loss in \"epoch_loss\".\n","  ###################\n","\n","\n","  ###################\n","  \n","  optimizer.step() # Updates the parameters.\n","\n","  instances_processed += batch_size\n","  if(instances_processed > epoch_size):\n","    print(f\"-- END OF EPOCH {epoch_id}.\")\n","    print(f\"Average loss: {sum(epoch_loss) / len(epoch_loss)}.\")\n","\n","    # Evaluation\n","    model.eval() # Tells Pytorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n","    with torch.no_grad(): # Deactivates Autograd (it is computationaly expensive and we don't need it here).\n","      accuracy = evaluation(\"train\")\n","      print(f\"Accuracy on the train set: {accuracy}.\")\n","\n","      accuracy = evaluation(\"dev\")\n","      print(f\"Accuracy on the dev set: {accuracy}.\")\n","\n","    epoch_id += 1\n","    instances_processed -= epoch_size\n","    epoch_loss = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zh4na1hivgXP"},"outputs":[],"source":["model.eval() # Tells Pytorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n","model(batch_generator.turn_into_batch([preprocess(text) for text in [\"This movie was terrible!!\", \"Pure gold!\", \"Bad.\", \"Not bad!\"]]).to(model.device))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1KPakgb8ETjP"},"outputs":[],"source":["# To go further\n","\n","# 1. Try to explain the results of the four reviews given in the cell above. Is there any weird behavior ? How could you explain it ?\n","# 2. You can try using longer GloVe vectors when creating the vocabulary (get_glove function). Beware : this will increase training time.\n","# 3. Feel free to play with the given hyperparams of the model to obtain a better accuracy.\n","# 4. Accuracy is not a really informative metric. Could you explain why ? If so, try to find a new metric and implement it."]}],"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3.8.15 ('nlp-env': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"f6921fcc8ce2fd7fdddc9a719d467b7c82af043e10fc90bc2c1513210b0f7025"}}},"nbformat":4,"nbformat_minor":0}
